@article{Mongillo2008,
author = {Gianluigi Mongillo  and Omri Barak  and Misha Tsodyks },
title = {Synaptic Theory of Working Memory},
journal = {Science},
volume = {319},
number = {5869},
pages = {1543-1546},
year = {2008},
doi = {10.1126/science.1150769},
URL = {https://www.science.org/doi/abs/10.1126/science.1150769},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1150769},
abstract = {It is usually assumed that enhanced spiking activity in the form of persistent reverberation for several seconds is the neural correlate of working memory. Here, we propose that working memory is sustained by calcium-mediated synaptic facilitation in the recurrent connections of neocortical networks. In this account, the presynaptic residual calcium is used as a buffer that is loaded, refreshed, and read out by spiking activity. Because of the long time constants of calcium kinetics, the refresh rate can be low, resulting in a mechanism that is metabolically efficient and robust. The duration and stability of working memory can be regulated by modulating the spontaneous activity in the network.}}


@article{tsodyks1998,
    author = {Tsodyks, Misha and Pawelzik, Klaus and Markram, Henry},
    title = "{Neural Networks with Dynamic Synapses}",
    journal = {Neural Computation},
    volume = {10},
    number = {4},
    pages = {821-835},
    year = {1998},
    month = {05},
    abstract = "{Transmission across neocortical synapses depends on the frequency of presynaptic activity (Thomson \\&amp; Deuchars, 1994). Interpyramidal synapses in layer V exhibit fast depression of synaptic transmission, while other types of synapses exhibit facilitation of transmission. To study the role of dynamic synapses in network computation, we propose a unified phenomenological model that allows computation of the postsynaptic current generated by both types of synapses when driven by an arbitrary pattern of action potential (AP) activity in a presynaptic population. Using this formalism, we analyze different regimes of synaptic transmission and demonstrate that dynamic synapses transmit different aspects of the presynaptic activity depending on the average presynaptic frequency. The model also allows for derivation of mean-field equations, which govern the activity of large, interconnected networks. We show that the dynamics of synaptic transmission results in complex sets of regular and irregular regimes of network activity.}",
    issn = {0899-7667},
    doi = {10.1162/089976698300017502},
    url = {https://doi.org/10.1162/089976698300017502},
    eprint = {https://direct.mit.edu/neco/article-pdf/10/4/821/813807/089976698300017502.pdf},
}


@article {tsodyks2000,
	author = {Tsodyks, Misha and Uziel, Asher and Markram, Henry},
	title = {Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses},
	volume = {20},
	number = {1},
	pages = {RC50--RC50},
	year = {2000},
	doi = {10.1523/JNEUROSCI.20-01-j0003.2000},
	publisher = {Society for Neuroscience},
	issn = {0270-6474},
	URL = {https://www.jneurosci.org/content/20/1/RC50},
	eprint = {https://www.jneurosci.org/content/20/1/RC50.full.pdf},
	journal = {Journal of Neuroscience}
}


@software{nest3.1,
  author       = {Deepu, Rajalekshmi and
                  Spreizer, Sebastian and
                  Trensch, Guido and
                  Terhorst, Dennis and
                  Vennemo, Stine Brekke and
                  Mitchell, Jessica and
                  Linssen, Charl and
                  Mørk, Håkon and
                  Morrison, Abigail and
                  Eppler, Jochen Martin and
                  Kamiji, Nilton Liuji and
                  de Schepper, Robin and
                  Kitayama, Itaru and
                  Kurth, Anno and
                  Morales-Gregorio, Aitor and
                  Nagendra Babu, Pooja and
                  Plesser, Hans Ekkehard},
  title        = {NEST 3.1},
  month        = sep,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {3.1},
  doi          = {10.5281/zenodo.5508805},
  url          = {https://doi.org/10.5281/zenodo.5508805}
}

@article{markram_wang_tsodyks1998,
author = {Henry Markram  and Yun Wang  and Misha Tsodyks },
title = {Differential signaling via the same axon of neocortical pyramidal neurons},
journal = {Proceedings of the National Academy of Sciences},
volume = {95},
number = {9},
pages = {5323-5328},
year = {1998},
doi = {10.1073/pnas.95.9.5323},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.95.9.5323},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.95.9.5323},
abstract = {The nature of information stemming from a single neuron and conveyed simultaneously to several hundred target neurons is not known. Triple and quadruple neuron recordings revealed that each synaptic connection established by neocortical pyramidal neurons is potentially unique. Specifically, synaptic connections onto the same morphological class differed in the numbers and dendritic locations of synaptic contacts, their absolute synaptic strengths, as well as their rates of synaptic depression and recovery from depression. The same axon of a pyramidal neuron innervating another pyramidal neuron and an interneuron mediated frequency-dependent depression and facilitation, respectively, during high frequency discharges of presynaptic action potentials, suggesting that the different natures of the target neurons underlie qualitative differences in synaptic properties. Facilitating-type synaptic connections established by three pyramidal neurons of the same class onto a single interneuron, were all qualitatively similar with a combination of facilitation and depression mechanisms. The time courses of facilitation and depression, however, differed for these convergent connections, suggesting that different pre-postsynaptic interactions underlie quantitative differences in synaptic properties. Mathematical analysis of the transfer functions of frequency-dependent synapses revealed supra-linear, linear, and sub-linear signaling regimes in which mixtures of presynaptic rates, integrals of rates, and derivatives of rates are transferred to targets depending on the precise values of the synaptic parameters and the history of presynaptic action potential activity. Heterogeneity of synaptic transfer functions therefore allows multiple synaptic representations of the same presynaptic action potential train and suggests that these synaptic representations are regulated in a complex manner. It is therefore proposed that differential signaling is a key mechanism in neocortical information processing, which can be regulated by selective synaptic modifications.}}


@article{barak_tsodyks2007,
    doi = {10.1371/journal.pcbi.0030035},
    author = {Barak, Omri AND Tsodyks, Misha},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Persistent Activity in Neural Networks with Dynamic Synapses},
    year = {2007},
    month = {02},
    volume = {3},
    url = {https://doi.org/10.1371/journal.pcbi.0030035},
    pages = {1-1},
    abstract = {Persistent activity states (attractors), observed in several neocortical areas after the removal of a sensory stimulus, are believed to be the neuronal basis of working memory. One of the possible mechanisms that can underlie persistent activity is recurrent excitation mediated by intracortical synaptic connections. A recent experimental study revealed that connections between pyramidal cells in prefrontal cortex exhibit various degrees of synaptic depression and facilitation. Here we analyze the effect of synaptic dynamics on the emergence and persistence of attractor states in interconnected neural networks. We show that different combinations of synaptic depression and facilitation result in qualitatively different network dynamics with respect to the emergence of the attractor states. This analysis raises the possibility that the framework of attractor neural networks can be extended to represent time-dependent stimuli.},
    number = {2},

}

@article{mi_katkov_tsodyks2017,
title = {Synaptic Correlates of Working Memory Capacity},
journal = {Neuron},
volume = {93},
number = {2},
pages = {323-330},
year = {2017},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2016.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0896627316309436},
author = {Yuanyuan Mi and Mikhail Katkov and Misha Tsodyks},
keywords = {neural networks, modeling, short-term memory, short-term synaptic plasticity, synaptic facilitation, synaptic depression, attractor neural networks, integrate and fire model, rate model},
abstract = {Summary
Psychological studies indicate that human ability to keep information in readily accessible working memory is limited to four items for most people. This extremely low capacity severely limits execution of many cognitive tasks, but its neuronal underpinnings remain unclear. Here we show that in the framework of synaptic theory of working memory, capacity can be analytically estimated to scale with characteristic time of short-term synaptic depression relative to synaptic current time constant. The number of items in working memory can be regulated by external excitation, enabling the system to be tuned to the desired load and to clear the working memory of currently held items to make room for new ones.}
}

@incollection{baddeley_hitch1974,
title = {Working Memory},
editor = {Gordon H. Bower},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {8},
pages = {47-89},
year = {1974},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60452-1},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108604521},
author = {Alan D. Baddeley and Graham Hitch},
abstract = {Publisher Summary
This chapter presents a body of new experimental evidence, which provides a firm basis for the working memory hypothesis. The chapter presents a series of experiments on the role of memory in reasoning, language comprehension, and learning. An attempt is made to apply the comparable techniques in all three cases to allow a common pattern to emerge, if the same working memory system is operative in all three instances. The chapter makes a case for postulating the working memory-LTS system as a modification of the current STS-LTS view. Working memory represents a control system with limits on both its storage and processing capabilities, and has access to phonemically coded information (possibly by controlling a rehearsal buffer), that it is responsible for the limited memory span, but does not underlie the recency effect in free recall. The experiments presented in the chapter suggest that the phonemic rehearsal buffer plays a limited role in this process, but is by no means essential. These experiments also suggest that working memory plays a part in verbal reasoning and in prose comprehension. Understanding the detailed role of working memory in these tasks, however, must proceed hand-in-hand with an understanding of the tasks themselves.}
}

@article{Miller1956,
  doi = {10.1037/h0043158},
  url = {https://doi.org/10.1037/h0043158},
  year = {1956},
  month = mar,
  publisher = {American Psychological Association ({APA})},
  volume = {63},
  number = {2},
  pages = {81--97},
  author = {George A. Miller},
  title = {The magical number seven,  plus or minus two: Some limits on our capacity for processing information.},
  journal = {Psychological Review}
}

@book{Miller1960,
  doi = {10.1037/10039-000},
  url = {https://doi.org/10.1037/10039-000},
  year = {1960},
  publisher = {Henry Holt and Co},
  author = {George A. Miller and Eugene Galanter and Karl H. Pribram},
  title = {Plans and the structure of behavior.}
}

@book{Cowan1998,
  doi = {10.1093/acprof:oso/9780195119107.001.0001},
  url = {https://doi.org/10.1093/acprof:oso/9780195119107.001.0001},
  year = {1998},
  month = feb,
  publisher = {Oxford University Press},
  author = {Nelson Cowan},
  title = {Attention and Memory}
}

@article{Oberauer2002,
  doi = {10.1037/0278-7393.28.3.411},
  url = {https://doi.org/10.1037/0278-7393.28.3.411},
  year = {2002},
  publisher = {American Psychological Association ({APA})},
  volume = {28},
  number = {3},
  pages = {411--421},
  author = {Klaus Oberauer},
  title = {Access to information in working memory: Exploring the focus of attention.},
  journal = {Journal of Experimental Psychology: Learning,  Memory,  and Cognition}
}

@article{GoldmanRakic1995,
  doi = {10.1016/0896-6273(95)90304-6},
  url = {https://doi.org/10.1016/0896-6273(95)90304-6},
  year = {1995},
  month = mar,
  publisher = {Elsevier {BV}},
  volume = {14},
  number = {3},
  pages = {477--485},
  author = {P.S Goldman-Rakic},
  title = {Cellular basis of working memory},
  journal = {Neuron}
}

@article{DEsposito2015,
  doi = {10.1146/annurev-psych-010814-015031},
  url = {https://doi.org/10.1146/annurev-psych-010814-015031},
  year = {2015},
  month = jan,
  publisher = {Annual Reviews},
  volume = {66},
  number = {1},
  pages = {115--142},
  author = {Mark D{\textquotesingle}Esposito and Bradley R. Postle},
  title = {The Cognitive Neuroscience of Working Memory},
  journal = {Annual Review of Psychology}
}

@article{Hansel2013,
  doi = {10.1523/jneurosci.3455-12.2013},
  url = {https://doi.org/10.1523/jneurosci.3455-12.2013},
  year = {2013},
  month = jan,
  publisher = {Society for Neuroscience},
  volume = {33},
  number = {1},
  pages = {133--149},
  author = {D. Hansel and G. Mato},
  title = {Short-Term Plasticity Explains Irregular Persistent Activity in Working Memory Tasks},
  journal = {Journal of Neuroscience}
}

@article{Lundqvist2016,
  doi = {10.1016/j.neuron.2016.02.028},
  url = {https://doi.org/10.1016/j.neuron.2016.02.028},
  year = {2016},
  month = apr,
  publisher = {Elsevier {BV}},
  volume = {90},
  number = {1},
  pages = {152--164},
  author = {Mikael Lundqvist and Jonas Rose and Pawel Herman and Scott~L. Brincat and Timothy~J. Buschman and Earl~K. Miller},
  title = {Gamma and Beta Bursts Underlie Working Memory},
  journal = {Neuron}
}

@article{Honkanen2014,
  doi = {10.1093/cercor/bhu263},
  url = {https://doi.org/10.1093/cercor/bhu263},
  year = {2014},
  month = nov,
  publisher = {Oxford University Press ({OUP})},
  volume = {25},
  number = {10},
  pages = {3788--3801},
  author = {Roosa Honkanen and Santeri Rouhinen and Sheng H. Wang and J. Matias Palva and Satu Palva},
  title = {Gamma Oscillations Underlie the Maintenance of Feature-Specific Information and the Contents of Visual Working Memory},
  journal = {Cerebral Cortex}
}

@article{Stokes2015,
  doi = {10.1016/j.tics.2015.05.004},
  url = {https://doi.org/10.1016/j.tics.2015.05.004},
  year = {2015},
  month = jul,
  publisher = {Elsevier {BV}},
  volume = {19},
  number = {7},
  pages = {394--405},
  author = {Mark G. Stokes},
  title = {`Activity-silent' working memory in prefrontal cortex: a dynamic coding framework},
  journal = {Trends in Cognitive Sciences}
}

@article{Funahashi1989,
  doi = {10.1152/jn.1989.61.2.331},
  url = {https://doi.org/10.1152/jn.1989.61.2.331},
  year = {1989},
  month = feb,
  publisher = {American Physiological Society},
  volume = {61},
  number = {2},
  pages = {331--349},
  author = {S. Funahashi and C. J. Bruce and P. S. Goldman-Rakic},
  title = {Mnemonic coding of visual space in the monkey{\textquotesingle}s dorsolateral prefrontal cortex},
  journal = {Journal of Neurophysiology}
}

@article{Hopfield1982,
  doi = {10.1073/pnas.79.8.2554},
  url = {https://doi.org/10.1073/pnas.79.8.2554},
  year = {1982},
  month = apr,
  publisher = {Proceedings of the National Academy of Sciences},
  volume = {79},
  number = {8},
  pages = {2554--2558},
  author = {J J Hopfield},
  title = {Neural networks and physical systems with emergent collective computational abilities.},
  journal = {Proceedings of the National Academy of Sciences}
}

@book{hebb-organization-of-behavior-1949,
  address = {New York},
  author = {Hebb, Donald O.},
  howpublished = {Hardcover},
  isbn = {0-8058-4300-0},
  publisher = {Wiley},
  title = {The organization of behavior: {A} neuropsychological
                 theory},
  year = 1949
}

@article{Rolls2013,
  doi = {10.1371/journal.pone.0061078},
  url = {https://doi.org/10.1371/journal.pone.0061078},
  year = {2013},
  month = apr,
  publisher = {Public Library of Science ({PLoS})},
  volume = {8},
  number = {4},
  pages = {e61078},
  author = {Edmund T. Rolls and Laura Dempere-Marco and Gustavo Deco},
  editor = {Sidney Arthur Simon},
  title = {Holding Multiple Items in Short Term Memory: A Neural Mechanism},
  journal = {{PLoS} {ONE}}
}

@article{Fiebig2017,
  doi = {10.1523/jneurosci.1989-16.2016},
  url = {https://doi.org/10.1523/jneurosci.1989-16.2016},
  year = {2016},
  month = nov,
  publisher = {Society for Neuroscience},
  volume = {37},
  number = {1},
  pages = {83--96},
  author = {Florian Fiebig and Anders Lansner},
  title = {A Spiking Working Memory Model Based on Hebbian Short-Term Potentiation},
  journal = {The Journal of Neuroscience}
}

@article{Fiebig2020,
  doi = {10.1523/eneuro.0374-19.2020},
  url = {https://doi.org/10.1523/eneuro.0374-19.2020},
  year = {2020},
  month = mar,
  publisher = {Society for Neuroscience},
  volume = {7},
  number = {2},
  pages = {ENEURO.0374--19.2020},
  author = {Florian Fiebig and Pawel Herman and Anders Lansner},
  title = {An Indexing Theory for Working Memory Based on Fast Hebbian Plasticity},
  journal = {eneuro}
}

@article{cowan_2001, 
    title={The magical number 4 in short-term memory: A reconsideration of mental storage capacity},
    volume={24}, 
    DOI={10.1017/S0140525X01003922}, 
    number={1}, 
    journal={Behavioral and Brain Sciences}, 
    publisher={Cambridge University Press}, 
    author={Cowan, Nelson}, 
    year={2001}, 
    pages={87–114}
}

@article{Cowan2010,
  doi = {10.1177/0963721409359277},
  url = {https://doi.org/10.1177/0963721409359277},
  year = {2010},
  month = feb,
  publisher = {{SAGE} Publications},
  volume = {19},
  number = {1},
  pages = {51--57},
  author = {Nelson Cowan},
  title = {The Magical Mystery Four},
  journal = {Current Directions in Psychological Science}
}

@article{Barak2014,
  doi = {10.1016/j.conb.2013.10.008},
  url = {https://doi.org/10.1016/j.conb.2013.10.008},
  year = {2014},
  month = apr,
  publisher = {Elsevier {BV}},
  volume = {25},
  pages = {20--24},
  author = {Omri Barak and Misha Tsodyks},
  title = {Working models of working memory},
  journal = {Current Opinion in Neurobiology}
}

@article{Golosio2015,
    doi = {10.1371/journal.pone.0140866},
    author = {Golosio, Bruno AND Cangelosi, Angelo AND Gamotina, Olesya AND Masala, Giovanni Luca},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {A Cognitive Neural Architecture Able to Learn and Communicate through Natural Language},
    year = {2015},
    month = {11},
    volume = {10},
    url = {https://doi.org/10.1371/journal.pone.0140866},
    pages = {1-37},
    abstract = {Communicative interactions involve a kind of procedural knowledge that is used by the human brain for processing verbal and nonverbal inputs and for language production. Although considerable work has been done on modeling human language abilities, it has been difficult to bring them together to a comprehensive tabula rasa system compatible with current knowledge of how verbal information is processed in the brain. This work presents a cognitive system, entirely based on a large-scale neural architecture, which was developed to shed light on the procedural knowledge involved in language elaboration. The main component of this system is the central executive, which is a supervising system that coordinates the other components of the working memory. In our model, the central executive is a neural network that takes as input the neural activation states of the short-term memory and yields as output mental actions, which control the flow of information among the working memory components through neural gating mechanisms. The proposed system is capable of learning to communicate through natural language starting from tabula rasa, without any a priori knowledge of the structure of phrases, meaning of words, role of the different classes of words, only by interacting with a human through a text-based interface, using an open-ended incremental learning process. It is able to learn nouns, verbs, adjectives, pronouns and other word classes, and to use them in expressive language. The model was validated on a corpus of 1587 input sentences, based on literature on early language assessment, at the level of about 4-years old child, and produced 521 output sentences, expressing a broad range of language processing functionalities.},
    number = {11},

}


@article{Rotter1999,
  doi = {10.1007/s004220050570},
  url = {https://doi.org/10.1007/s004220050570},
  year = {1999},
  month = nov,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {81},
  number = {5-6},
  pages = {381--402},
  author = {Stefan Rotter and Markus Diesmann},
  title = {Exact digital simulation of time-invariant linear systems with applications to neuronal modeling},
  journal = {Biological Cybernetics}
}

@article{Gast2021,
  title = {Mean-field approximations of networks of spiking neurons with short-term synaptic plasticity},
  author = {Gast, Richard and Kn\"osche, Thomas R. and Schmidt, Helmut},
  journal = {Phys. Rev. E},
  volume = {104},
  issue = {4},
  pages = {044310},
  numpages = {15},
  year = {2021},
  month = {Oct},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.104.044310},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.104.044310}
}

@article{Shafi2007,
title = {Variability in neuronal activity in primate cortex during working memory tasks},
journal = {Neuroscience},
volume = {146},
number = {3},
pages = {1082-1108},
year = {2007},
issn = {0306-4522},
doi = {https://doi.org/10.1016/j.neuroscience.2006.12.072},
url = {https://www.sciencedirect.com/science/article/pii/S0306452206017593},
author = {M. Shafi and Y. Zhou and J. Quintana and C. Chow and J. Fuster and M. Bodner},
keywords = {spike trains, monkeys, memory networks, parietal cortex, prefrontal cortex, computational models},
abstract = {Persistent elevated neuronal activity has been identified as the neuronal correlate of working memory. It is generally assumed in the literature and in computational and theoretical models of working memory that memory-cell activity is stable and replicable; however, this assumption may be an artifact of the averaging of data collected across trials, and needs experimental verification. In this study, we introduce a classification scheme to characterize the firing frequency trends of cells recorded from the cortex of monkeys during performance of working memory tasks. We examine the frequency statistics and variability of firing during baseline and memory periods. We also study the behavior of cells on individual trials and across trials, and explore the stability of cellular firing during the memory period. We find that cells from different firing-trend classes possess markedly different statistics. We also find that individual cells show substantial variability in their firing behavior across trials, and that firing frequency also varies markedly over the course of a single trial. Finally, the average frequency distribution is wider, the magnitude of the frequency increases from baseline to memory smaller, and the magnitude of frequency decreases larger than is generally assumed. These results may serve as a guide in the evaluation of current theories of the cortical mechanisms of working memory.}
}

@article{Brunel2000,
author = {Brunel, Nicolas},
year = {2000},
month = {01},
pages = {},
title = {Persistent Activity and the Single Cell Frequency-Current Curve in a Cortical Network Model},
volume = {11},
journal = {Network Computation in Neural Systems},
doi = {10.1088/0954-898X/11/4/302}
}

@Inbook{Barri2022,
author="Barri, Alessandro
and Mongillo, Gianluigi",
editor="Giugliano, Michele
and Negrello, Mario
and Linaro, Daniele",
title="Short-Term Synaptic Plasticity: Microscopic Modelling and (Some) Computational Implications",
bookTitle="Computational Modelling of the Brain: Modelling Approaches to Cells, Circuits and Networks",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="105--121",
abstract="Synaptic transmission is transiently adjusted on a spike-by-spike basis, with the adjustments persisting from hundreds of milliseconds up to seconds. Such a short-term plasticity has been suggested to significantly augment the computational capabilities of neuronal networks by enhancing their dynamical repertoire. In this chapter, after reviewing the basic physiology of chemical synaptic transmission, we present a general framework---inspired by the quantal model---to build simple, yet quantitatively accurate models of repetitive synaptic transmission. We also discuss different methods to obtain estimates of the model's parameters from experimental recordings. Next, we show that, indeed, new dynamical regimes appear in the presence of short-term synaptic plasticity. In particular, model neuronal networks exhibit the co-existence of a stable fixed point and a stable limit cycle in the presence of short-term synaptic facilitation. It has been suggested that this dynamical regime is especially relevant in working memory processes. We provide, then, a short summary of the synaptic theory of working memory and discuss some of its specific predictions in the context of experiments. We conclude the chapter with a short outlook.",
isbn="978-3-030-89439-9",
doi="10.1007/978-3-030-89439-9_5",
url="https://doi.org/10.1007/978-3-030-89439-9_5"
}

@Article{Wang2006,
author={Wang, Yun
and Markram, Henry
and Goodman, Philip H.
and Berger, Thomas K.
and Ma, Junying
and Goldman-Rakic, Patricia S.},
title={Heterogeneity in the pyramidal network of the medial prefrontal cortex},
journal={Nature Neuroscience},
year={2006},
month={Apr},
day={01},
volume={9},
number={4},
pages={534-542},
abstract={The prefrontal cortex is specially adapted to generate persistent activity that outlasts stimuli and is resistant to distractors, presumed to be the basis of working memory. The pyramidal network that supports this activity is unknown. Multineuron patch-clamp recordings in the ferret medial prefrontal cortex showed a heterogeneity of synapses interconnecting distinct subnetworks of different pyramidal cells. One subnetwork was similar to the pyramidal network commonly found in primary sensory areas, consisting of accommodating pyramidal cells interconnected with depressing synapses. The other subnetwork contained complex pyramidal cells with dual apical dendrites displaying nonaccommodating discharge patterns; these cells were hyper-reciprocally connected with facilitating synapses displaying pronounced synaptic augmentation and post-tetanic potentiation. These cellular, synaptic and network properties could amplify recurrent interactions between pyramidal neurons and support persistent activity in the prefrontal cortex.},
issn={1546-1726},
doi={10.1038/nn1670},
url={https://doi.org/10.1038/nn1670}
}

@ARTICLE{Wolff2015,
  
AUTHOR={Wolff, Michael and Ding, Jacqueline and Myers, Nicholas and Stokes, Mark},   
	 
TITLE={Revealing hidden states in visual working memory using electroencephalography},      
	
JOURNAL={Frontiers in Systems Neuroscience},      
	
VOLUME={9},           
	
YEAR={2015},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fnsys.2015.00123},       
	
DOI={10.3389/fnsys.2015.00123},      
	
ISSN={1662-5137},   
   
ABSTRACT={It is often assumed that information in visual working memory (vWM) is maintained via persistent activity. However, recent evidence indicates that information in vWM could be maintained in an effectively “activity-silent” neural state. Silent vWM is consistent with recent cognitive and neural models, but poses an important experimental problem: how can we study these silent states using conventional measures of brain activity? We propose a novel approach that is analogous to echolocation: using a high-contrast visual stimulus, it may be possible to drive brain activity during vWM maintenance and measure the vWM-dependent impulse response. We recorded electroencephalography (EEG) while participants performed a vWM task in which a randomly oriented grating was remembered. Crucially, a high-contrast, task-irrelevant stimulus was shown in the maintenance period in half of the trials. The electrophysiological response from posterior channels was used to decode the orientations of the gratings. While orientations could be decoded during and shortly after stimulus presentation, decoding accuracy dropped back close to baseline in the delay. However, the visual evoked response from the task-irrelevant stimulus resulted in a clear re-emergence in decodability. This result provides important proof-of-concept for a promising and relatively simple approach to decode “activity-silent” vWM content using non-invasive EEG.}
}

@Article{Wolff2017,
author={Wolff, Michael J.
and Jochim, Janina
and Aky{\"u}rek, Elkan G.
and Stokes, Mark G.},
title={Dynamic hidden states underlying working-memory-guided behavior},
journal={Nature Neuroscience},
year={2017},
month={Jun},
day={01},
volume={20},
number={6},
pages={864-871},
abstract={Wolff and colleagues show that `activity-silent' brain states are important to working memory. Using a perturbation method to `ping' the brain, they uncover hidden neural states that reflect temporary information held in mind and predict memory performance. They argue that dynamic hidden states could underpin working memory.},
issn={1546-1726},
doi={10.1038/nn.4546},
url={https://doi.org/10.1038/nn.4546}
}

@article{Rose2016,
author = {Nathan S. Rose  and Joshua J. LaRocque  and Adam C. Riggall  and Olivia Gosseries  and Michael J. Starrett  and Emma E. Meyering  and Bradley R. Postle },
title = {Reactivation of latent working memories with transcranial magnetic stimulation},
journal = {Science},
volume = {354},
number = {6316},
pages = {1136-1139},
year = {2016},
doi = {10.1126/science.aah7011},
URL = {https://www.science.org/doi/abs/10.1126/science.aah7011},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aah7011},
abstract = {A pulse of random activity is sufficient for a brain network to retrieve a dormant activity state. Sophisticated techniques can decode stimulus representations for items held in a person's working memory. However, when subjects shift their attention toward something else, the neural representation of the now unattended item drops to baseline, as though the item has been forgotten. Rose et al. used single-pulse transcranial magnetic stimulation (TMS) to briefly reactivate the representation of an unattended item. A short pulse of TMS enhanced recognition of “forgotten” stimuli, bringing an unattended item back into focal attention. Science, this issue p. 1136 The ability to hold information in working memory is fundamental for cognition. Contrary to the long-standing view that working memory depends on sustained, elevated activity, we present evidence suggesting that humans can hold information in working memory via “activity-silent” synaptic mechanisms. Using multivariate pattern analyses to decode brain activity patterns, we found that the active representation of an item in working memory drops to baseline when attention shifts away. A targeted pulse of transcranial magnetic stimulation produced a brief reemergence of the item in concurrently measured brain activity. This reactivation effect occurred and influenced memory performance only when the item was potentially relevant later in the trial, which suggests that the representation is dynamic and modifiable via cognitive control. The results support a synaptic theory of working memory.}}

@Article{Kilpatrick2018,
author={Kilpatrick, Zachary P.},
title={Synaptic mechanisms of interference in working memory},
journal={Scientific Reports},
year={2018},
month={May},
day={18},
volume={8},
number={1},
pages={7879},
abstract={Information from preceding trials of cognitive tasks can bias performance in the current trial, a phenomenon referred to as interference. Subjects performing visual working memory tasks exhibit interference in their responses: the recalled target location is biased in the direction of the target presented on the previous trial. We present modeling work that develops a probabilistic inference model of this history-dependent bias, and links our probabilistic model to computations of a recurrent network wherein short-term facilitation accounts for the observed bias. Network connectivity is reshaped dynamically during each trial, generating predictions from prior trial observations. Applying timescale separation methods, we obtain a low-dimensional description of the trial-to-trial bias based on the history of target locations. Furthermore, we demonstrate task protocols for which our model with facilitation performs better than a model with static connectivity: repetitively presented targets are better retained in working memory than targets drawn from uncorrelated sequences.},
issn={2045-2322},
doi={10.1038/s41598-018-25958-9},
url={https://doi.org/10.1038/s41598-018-25958-9}
}

@ARTICLE{Kiyonaga2017,
  title     = "Serial dependence across perception, attention, and memory",
  author    = "Kiyonaga, Anastasia and Scimeca, Jason M and Bliss, Daniel P and
               Whitney, David",
  journal   = "Trends Cogn. Sci.",
  publisher = "Elsevier BV",
  volume    =  21,
  number    =  7,
  pages     = "493--497",
  month     =  jul,
  year      =  2017
}

@ARTICLE{Barbosa2020,
  title    = "Interplay between persistent activity and activity-silent
              dynamics in the prefrontal cortex underlies serial biases in
              working memory",
  author   = "Barbosa, Joao and Stein, Heike and Martinez, Rebecca L and
              Galan-Gadea, Adri{\`a} and Li, Sihai and Dalmau, Josep and Adam,
              Kirsten C S and Valls-Sol{\'e}, Josep and Constantinidis,
              Christos and Compte, Albert",
  abstract = "Persistent neuronal spiking has long been considered the
              mechanism underlying working memory, but recent proposals argue
              for alternative `activity-silent' substrates. Using monkey and
              human electrophysiology data, we show here that attractor
              dynamics that control neural spiking during mnemonic periods
              interact with activity-silent mechanisms in the prefrontal cortex
              (PFC). This interaction allows memory reactivations, which
              enhance serial biases in spatial working memory. Stimulus
              information was not decodable between trials, but remained
              present in activity-silent traces inferred from spiking synchrony
              in the PFC. Just before the new stimulus, this latent trace was
              reignited into activity that recapitulated the previous stimulus
              representation. Importantly, the reactivation strength correlated
              with the strength of serial biases in both monkeys and humans, as
              predicted by a computational model that integrates activity-based
              and activity-silent mechanisms. Finally, single-pulse
              transcranial magnetic stimulation applied to the human PFC
              between successive trials enhanced serial biases, thus
              demonstrating the causal role of prefrontal reactivations in
              determining working-memory behavior.",
  journal  = "Nature Neuroscience",
  volume   =  23,
  number   =  8,
  pages    = "1016--1024",
  month    =  aug,
  year     =  2020
}


@article{Cortes2013,
author = {Jesus M. Cortes  and Mathieu Desroches  and Serafim Rodrigues  and Romain Veltz  and Miguel A. Muñoz  and Terrence J. Sejnowski },
title = {Short-term synaptic plasticity in the deterministic {T}sodyks-{M}arkram model leads to unpredictable network dynamics},
journal = {Proceedings of the National Academy of Sciences},
volume = {110},
number = {41},
pages = {16610-16615},
year = {2013},
doi = {10.1073/pnas.1316071110},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1316071110},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1316071110},
abstract = {Short-term synaptic plasticity contributes to the balance and regulation of brain networks from milliseconds to several minutes. In this paper we report the existence of a route to chaos in the Tsodyks and Markram model of short-term synaptic plasticity. The chaotic region corresponds to what in mathematics is called Shilnikov chaos, an unstable manifold that strongly modifies the shape of trajectories and induces highly irregular transient dynamics, even in the absence of noise. The interplay between the Shilnikov chaos and stochastic effects may give rise to some of the complex dynamics observed in neural systems such as transitions between up and down states. Short-term synaptic plasticity strongly affects the neural dynamics of cortical networks. The Tsodyks and Markram (TM) model for short-term synaptic plasticity accurately accounts for a wide range of physiological responses at different types of cortical synapses. Here, we report a route to chaotic behavior via a Shilnikov homoclinic bifurcation that dynamically organizes some of the responses in the TM model. In particular, the presence of such a homoclinic bifurcation strongly affects the shape of the trajectories in the phase space and induces highly irregular transient dynamics; indeed, in the vicinity of the Shilnikov homoclinic bifurcation, the number of population spikes and their precise timing are unpredictable and highly sensitive to the initial conditions. Such an irregular deterministic dynamics has its counterpart in stochastic/network versions of the TM model: The existence of the Shilnikov homoclinic bifurcation generates complex and irregular spiking patterns and—acting as a sort of springboard—facilitates transitions between the down-state and unstable periodic orbits. The interplay between the (deterministic) homoclinic bifurcation and stochastic effects may give rise to some of the complex dynamics observed in neural systems.}}

@article{Coultrip1992,
title = {A cortical model of winner-take-all competition via lateral inhibition},
journal = {Neural Networks},
volume = {5},
number = {1},
pages = {47-54},
year = {1992},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(05)80006-1},
url = {https://www.sciencedirect.com/science/article/pii/S0893608005800061},
author = {Robert Coultrip and Richard Granger and Gary Lynch},
keywords = {Winner-take-all circuits, Lateral inhibition},
abstract = {Simulations were performed of physiological interactions among excitatory and inhibitory neurons in anatomically realistic local-circuit architectures modeled after hippocampal field CA1. The simulated circuitry consists of several excitatory neurons jointly innervating and receiving feedback from a common inhibitory interneuron. Excitatory cells in the simulation receive input during a cycle of naturally-occurring rhythmic activity (the hippocampal theta rhythm), and the neuron receiving the most input activation is the first to reach its spiking threshold. Spiking excites the inhibitory cell, which in turn prevents other cells from responding. The result is the natural generation of a simple competitive or “winner-take-all” (WTA) mechanism, allowing only the most strongly-activated cell in a group or “patch” to respond with spiking activity. Formal mathematical characterization of the mechanism reveals specific physiological characteristics of the input to the network, which enable it to closely approximate an ideal winner-take-all mechanism. Unlike other, more abstract WTA mechanisms that have been proposed, the parameters of this biologically-derived WTA mechanism can be directly related to specific physiological and anatomical features of particular cortical circuits.}
}

@article{Taher2020,
    doi = {10.1371/journal.pcbi.1008533},
    author = {Taher, Halgurd AND Torcini, Alessandro AND Olmi, Simona},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Exact neural mass model for synaptic-based working memory},
    year = {2020},
    month = {12},
    volume = {16},
    url = {https://doi.org/10.1371/journal.pcbi.1008533},
    pages = {1-42},
    abstract = {A synaptic theory of Working Memory (WM) has been developed in the last decade as a possible alternative to the persistent spiking paradigm. In this context, we have developed a neural mass model able to reproduce exactly the dynamics of heterogeneous spiking neural networks encompassing realistic cellular mechanisms for short-term synaptic plasticity. This population model reproduces the macroscopic dynamics of the network in terms of the firing rate and the mean membrane potential. The latter quantity allows us to gain insight of the Local Field Potential and electroencephalographic signals measured during WM tasks to characterize the brain activity. More specifically synaptic facilitation and depression integrate each other to efficiently mimic WM operations via either synaptic reactivation or persistent activity. Memory access and loading are related to stimulus-locked transient oscillations followed by a steady-state activity in the β-γ band, thus resembling what is observed in the cortex during vibrotactile stimuli in humans and object recognition in monkeys. Memory juggling and competition emerge already by loading only two items. However more items can be stored in WM by considering neural architectures composed of multiple excitatory populations and a common inhibitory pool. Memory capacity depends strongly on the presentation rate of the items and it maximizes for an optimal frequency range. In particular we provide an analytic expression for the maximal memory capacity. Furthermore, the mean membrane potential turns out to be a suitable proxy to measure the memory load, analogously to event driven potentials in experiments on humans. Finally we show that the γ power increases with the number of loaded items, as reported in many experiments, while θ and β power reveal non monotonic behaviours. In particular, β and γ rhythms are crucially sustained by the inhibitory activity, while the θ rhythm is controlled by excitatory synapses.},
    number = {12},

}

@article{Fino2011,
title = {Dense Inhibitory Connectivity in Neocortex},
journal = {Neuron},
volume = {69},
number = {6},
pages = {1188-1203},
year = {2011},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2011.02.025},
url = {https://www.sciencedirect.com/science/article/pii/S0896627311001231},
author = {Elodie Fino and Rafael Yuste},
abstract = {Summary
The connectivity diagram of neocortical circuits is still unknown, and there are conflicting data as to whether cortical neurons are wired specifically or not. To investigate the basic structure of cortical microcircuits, we use a two-photon photostimulation technique that enables the systematic mapping of synaptic connections with single-cell resolution. We map the inhibitory connectivity between upper layers somatostatin-positive GABAergic interneurons and pyramidal cells in mouse frontal cortex. Most, and sometimes all, inhibitory neurons are locally connected to every sampled pyramidal cell. This dense inhibitory connectivity is found at both young and mature developmental ages. Inhibitory innervation of neighboring pyramidal cells is similar, regardless of whether they are connected among themselves or not. We conclude that local inhibitory connectivity is promiscuous, does not form subnetworks, and can approach the theoretical limit of a completely connected synaptic matrix.}
}


@Article{Burkitt2006,
author={Burkitt, A. N.},
title={A Review of the Integrate-and-fire Neuron Model: I. Homogeneous Synaptic Input},
journal={Biological Cybernetics},
year={2006},
month={Jul},
day={01},
volume={95},
number={1},
pages={1-19},
abstract={The integrate-and-fire neuron model is one of the most widely used models for analyzing the behavior of neural systems. It describes the membrane potential of a neuron in terms of the synaptic inputs and the injected current that it receives. An action potential (spike) is generated when the membrane potential reaches a threshold, but the actual changes associated with the membrane voltage and conductances driving the action potential do not form part of the model. The synaptic inputs to the neuron are considered to be stochastic and are described as a temporally homogeneous Poisson process. Methods and results for both current synapses and conductance synapses are examined in the diffusion approximation, where the individual contributions to the postsynaptic potential are small. The focus of this review is upon the mathematical techniques that give the time distribution of output spikes, namely stochastic differential equations and the Fokker--Planck equation. The integrate-and-fire neuron model has become established as a canonical model for the description of spiking neurons because it is capable of being analyzed mathematically while at the same time being sufficiently complex to capture many of the essential features of neural processing. A number of variations of the model are discussed, together with the relationship with the Hodgkin--Huxley neuron model and the comparison with electrophysiological data. A brief overview is given of two issues in neural information processing that the integrate-and-fire neuron model has contributed to -- the irregular nature of spiking in cortical neurons and neural gain modulation.},
issn={1432-0770},
doi={10.1007/s00422-006-0068-6},
url={https://doi.org/10.1007/s00422-006-0068-6}
}

@ARTICLE{Hanuschkin2010,
  
AUTHOR={Hanuschkin, Alexander and Kunkel, Susanne and Helias, Moritz and Morrison, Abigail and Diesmann, Markus},   
	 
TITLE={A General and Efficient Method for Incorporating Precise Spike Times in Globally Time-Driven Simulations},      
	
JOURNAL={Frontiers in Neuroinformatics},      
	
VOLUME={4},           
	
YEAR={2010},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fninf.2010.00113},       
	
DOI={10.3389/fninf.2010.00113},      
	
ISSN={1662-5196},   
   
ABSTRACT={Traditionally, event-driven simulations have been limited to the very restricted class of neuronal models for which the timing of future spikes can be expressed in closed form. Recently, the class of models that is amenable to event-driven simulation has been extended by the development of techniques to accurately calculate firing times for some integrate-and-fire neuron models that do not enable the prediction of future spikes in closed form. The motivation of this development is the general perception that time-driven simulations are imprecise. Here, we demonstrate that a globally time-driven scheme can calculate firing times that cannot be discriminated from those calculated by an event-driven implementation of the same model; moreover, the time-driven scheme incurs lower computational costs. The key insight is that time-driven methods are based on identifying a threshold crossing in the recent past, which can be implemented by a much simpler algorithm than the techniques for predicting future threshold crossings that are necessary for event-driven approaches. As run time is dominated by the cost of the operations performed at each incoming spike, which includes spike prediction in the case of event-driven simulation and retrospective detection in the case of time-driven simulation, the simple time-driven algorithm outperforms the event-driven approaches. Additionally, our method is generally applicable to all commonly used integrate-and-fire neuronal models; we show that a non-linear model employing a standard adaptive solver can reproduce a reference spike train with a high degree of precision.}
}

@ARTICLE{Balkenius2018,
  
AUTHOR={Balkenius, Christian and Tjøstheim, Trond A. and Johansson, Birger and Gärdenfors, Peter},   
	 
TITLE={From Focused Thought to Reveries: A Memory System for a Conscious Robot},      
	
JOURNAL={Frontiers in Robotics and AI},      
	
VOLUME={5},           
	
YEAR={2018},      
	  
URL={https://www.frontiersin.org/articles/10.3389/frobt.2018.00029},       
	
DOI={10.3389/frobt.2018.00029},      
	
ISSN={2296-9144},   
   
ABSTRACT={We introduce a memory model for robots that can account for many aspects of an inner world, ranging from object permanence, episodic memory, and planning to imagination and reveries. It is modeled after neurophysiological data and includes parts of the cerebral cortex together with models of arousal systems that are relevant for consciousness. The three central components are an identification network, a localization network, and a working memory network. Attention serves as the interface between the inner and the external world. It directs the flow of information from sensory organs to memory, as well as controlling top-down influences on perception. It also compares external sensations to internal top-down expectations. The model is tested in a number of computer simulations that illustrate how it can operate as a component in various cognitive tasks including perception, the A-not-B test, delayed matching to sample, episodic recall, and vicarious trial and error.}
}

@ARTICLE{Giorgi2021,

  author={Giorgi, Ioanna and Golosio, Bruno and Esposito, Massimo and Cangelosi, Angelo and Masala, Giovanni L.},

  journal={IEEE Transactions on Cognitive and Developmental Systems}, 

  title={Modeling Multiple Language Learning in a Developmental Cognitive Architecture}, 

  year={2021},

  volume={13},

  number={4},

  pages={922-933},

  doi={10.1109/TCDS.2020.3033963}}


@ARTICLE{Giorgi2021b,
  
AUTHOR={Giorgi, Ioanna and Cangelosi, Angelo and Masala, Giovanni L.},   
	 
TITLE={Learning Actions From Natural Language Instructions Using an ON-World Embodied Cognitive Architecture},      
	
JOURNAL={Frontiers in Neurorobotics},      
	
VOLUME={15},           
	
YEAR={2021},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fnbot.2021.626380},       
	
DOI={10.3389/fnbot.2021.626380},      
	
ISSN={1662-5218},   
   
ABSTRACT={Endowing robots with the ability to view the world the way humans do, to understand natural language and to learn novel semantic meanings when they are deployed in the physical world, is a compelling problem. Another significant aspect is linking language to action, in particular, utterances involving abstract words, in artificial agents. In this work, we propose a novel methodology, using a brain-inspired architecture, to model an appropriate mapping of language with the percept and internal motor representation in humanoid robots. This research presents the first robotic instantiation of a complex architecture based on the Baddeley's Working Memory (WM) model. Our proposed method grants a scalable knowledge representation of verbal and non-verbal signals in the cognitive architecture, which supports incremental open-ended learning. Human spoken utterances about the workspace and the task are combined with the internal knowledge map of the robot to achieve task accomplishment goals. We train the robot to understand instructions involving higher-order (abstract) linguistic concepts of developmental complexity, which cannot be directly hooked in the physical world and are not pre-defined in the robot's static self-representation. Our proposed interactive learning method grants flexible run-time acquisition of novel linguistic forms and real-world information, without training the cognitive model anew. Hence, the robot can adapt to new workspaces that include novel objects and task outcomes. We assess the potential of the proposed methodology in verification experiments with a humanoid robot. The obtained results suggest robust capabilities of the model to link language bi-directionally with the physical environment and solve a variety of manipulation tasks, starting with limited knowledge and gradually learning from the run-time interaction with the tutor, past the pre-trained stage.}
}

@article{Roberts2012,
  doi = {10.48550/ARXIV.1210.0933},
  url = {https://arxiv.org/abs/1210.0933},
  author = {Roberts, A. J.},
  keywords = {Numerical Analysis (math.NA), FOS: Mathematics, FOS: Mathematics},
  title = {Modify the Improved {E}uler scheme to integrate stochastic differential equations},
  publisher = {arXiv},
  year = {2012},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{Machevicisu2011,
publisher = {John Wiley \& Sons, Ltd},
author = {Vigirdas Mackevičius},
isbn = {9781118603338},
title = {Introduction to Stochastic Analysis},
pages = {217-250},
doi = {https://doi.org/10.1002/9781118603338},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118603338},
year = {2011}
}

@article{vanKampen1981,
  doi = {10.1007/bf01007642},
  url = {https://doi.org/10.1007/bf01007642},
  year = {1981},
  month = jan,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {24},
  number = {1},
  pages = {175--187},
  author = {N. G. van Kampen},
  title = {It{\^{o}} versus {S}tratonovich},
  journal = {Journal of Statistical Physics}
}

@article{Golosio2021-ThaCo,
    doi = {10.1371/journal.pcbi.1009045},
    author = {Golosio, Bruno AND De Luca, Chiara AND Capone, Cristiano AND Pastorelli, Elena AND Stegel, Giovanni AND Tiddia, Gianmarco AND De Bonis, Giulia AND Paolucci, Pier Stanislao},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Thalamo-cortical spiking model of incremental learning combining perception, context and NREM-sleep},
    year = {2021},
    month = {06},
    volume = {17},
    url = {https://doi.org/10.1371/journal.pcbi.1009045},
    pages = {1-26},
    abstract = {The brain exhibits capabilities of fast incremental learning from few noisy examples, as well as the ability to associate similar memories in autonomously-created categories and to combine contextual hints with sensory perceptions. Together with sleep, these mechanisms are thought to be key components of many high-level cognitive functions. Yet, little is known about the underlying processes and the specific roles of different brain states. In this work, we exploited the combination of context and perception in a thalamo-cortical model based on a soft winner-take-all circuit of excitatory and inhibitory spiking neurons. After calibrating this model to express awake and deep-sleep states with features comparable with biological measures, we demonstrate the model capability of fast incremental learning from few examples, its resilience when proposed with noisy perceptions and contextual signals, and an improvement in visual classification after sleep due to induced synaptic homeostasis and association of similar memories.},
    number = {6},
}


@misc{DeLuca2023,
      title={NREM and REM: cognitive and energetic gains in thalamo-cortical sleeping and awake spiking model}, 
      author={Chiara De Luca and Leonardo Tonielli and Elena Pastorelli and Cristiano Capone and Francesco Simula and Cosimo Lupo and Irene Bernava and Giulia De Bonis and Gianmarco Tiddia and Bruno Golosio and Pier Stanislao Paolucci},
      year={2023},
      eprint={2211.06889},
      archivePrefix={arXiv},
      primaryClass={q-bio.NC}
}


@Article{Capone2019,
author={Capone, Cristiano
and Pastorelli, Elena
and Golosio, Bruno
and Paolucci, Pier Stanislao},
title={Sleep-like slow oscillations improve visual classification through synaptic homeostasis and memory association in a thalamo-cortical model},
journal={Scientific Reports},
year={2019},
month={Jun},
day={20},
volume={9},
number={1},
pages={8990},
abstract={The occurrence of sleep passed through the evolutionary sieve and is widespread in animal species. Sleep is known to be beneficial to cognitive and mnemonic tasks, while chronic sleep deprivation is detrimental. Despite the importance of the phenomenon, a complete understanding of its functions and underlying mechanisms is still lacking. In this paper, we show interesting effects of deep-sleep-like slow oscillation activity on a simplified thalamo-cortical model which is trained to encode, retrieve and classify images of handwritten digits. During slow oscillations, spike-timing-dependent-plasticity (STDP) produces a differential homeostatic process. It is characterized by both a specific unsupervised enhancement of connections among groups of neurons associated to instances of the same class (digit) and a simultaneous down-regulation of stronger synapses created by the training. This hierarchical organization of post-sleep internal representations favours higher performances in retrieval and classification tasks. The mechanism is based on the interaction between top-down cortico-thalamic predictions and bottom-up thalamo-cortical projections during deep-sleep-like slow oscillations. Indeed, when learned patterns are replayed during sleep, cortico-thalamo-cortical connections favour the activation of other neurons coding for similar thalamic inputs, promoting their association. Such mechanism hints at possible applications to artificial learning systems.},
issn={2045-2322},
doi={10.1038/s41598-019-45525-0},
url={https://doi.org/10.1038/s41598-019-45525-0}
}

@book{dyn_sys_neurosc,
  doi = {10.7551/mitpress/2526.001.0001},
  url = {https://doi.org/10.7551/mitpress/2526.001.0001},
  year = {2006},
  publisher = {The {MIT} Press},
  author = {Eugene M. Izhikevich},
  title = {Dynamical Systems in Neuroscience}
}

@book{neuronal_dynamics,
    place={Cambridge},
    title={Neuronal Dynamics: From Single Neurons to Networks and Models of Cognition}, DOI={10.1017/CBO9781107447615},
    publisher={Cambridge University Press},
    author={Gerstner, Wulfram and Kistler, Werner M. and Naud, Richard and Paninski, Liam},
    year={2014}
}

@incollection{modelling_synapses,
    author = {Roth, Arnd and van Rossum, Mark C. W.},
    isbn = {9780262013277},
    title = "{139Modeling Synapses}",
    booktitle = "{Computational Modeling Methods for Neuroscientists}",
    publisher = {The MIT Press},
    year = {2009},
    month = {09},
    abstract = "{This chapter discusses the models at different levels of realism and computational efficiency for simulating synapses and their plasticity. It outlines the experimental data that offer the parameter values for the synapse models, concentrating on the dominant transmitter and receptor types mediating fast synaptic transmission in the mammalian central nervous system. The underlying mechanisms of synaptic transmission are analyzed, and simple stochastic descriptions of transmitter release, stochastic modeling of transmitter diffusion, and stochastic models of channel gating are presented. The chapter shows some quite simple formulations that offer decent models for the synaptic current.}",
    doi = {10.7551/mitpress/9780262013277.003.0007},
    url = {https://doi.org/10.7551/mitpress/9780262013277.003.0007},
    eprint = {https://academic.oup.com/mit-press-scholarship-online/book/0/chapter/184559351/chapter-ag-pdf/44669960/book\_23468\_section\_184559351.ag.pdf},
}

@article{Senk2022,
    doi = {10.1371/journal.pcbi.1010086},
    author = {Senk, Johanna AND Kriener, Birgit AND Djurfeldt, Mikael AND Voges, Nicole AND Jiang, Han-Jia AND Schüttler, Lisa AND Gramelsberger, Gabriele AND Diesmann, Markus AND Plesser, Hans E. AND van Albada, Sacha J.},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Connectivity concepts in neuronal network modeling},
    year = {2022},
    month = {09},
    volume = {18},
    url = {https://doi.org/10.1371/journal.pcbi.1010086},
    pages = {1-49},
    abstract = {Sustainable research on computational models of neuronal networks requires published models to be understandable, reproducible, and extendable. Missing details or ambiguities about mathematical concepts and assumptions, algorithmic implementations, or parameterizations hinder progress. Such flaws are unfortunately frequent and one reason is a lack of readily applicable standards and tools for model description. Our work aims to advance complete and concise descriptions of network connectivity but also to guide the implementation of connection routines in simulation software and neuromorphic hardware systems. We first review models made available by the computational neuroscience community in the repositories ModelDB and Open Source Brain, and investigate the corresponding connectivity structures and their descriptions in both manuscript and code. The review comprises the connectivity of networks with diverse levels of neuroanatomical detail and exposes how connectivity is abstracted in existing description languages and simulator interfaces. We find that a substantial proportion of the published descriptions of connectivity is ambiguous. Based on this review, we derive a set of connectivity concepts for deterministically and probabilistically connected networks and also address networks embedded in metric space. Beside these mathematical and textual guidelines, we propose a unified graphical notation for network diagrams to facilitate an intuitive understanding of network properties. Examples of representative network models demonstrate the practical use of the ideas. We hope that the proposed standardizations will contribute to unambiguous descriptions and reproducible implementations of neuronal network connectivity in computational neuroscience.},
    number = {9},

}

@ARTICLE{Golosio2021,
  
AUTHOR={Golosio, Bruno and Tiddia, Gianmarco and De Luca, Chiara and Pastorelli, Elena and Simula, Francesco and Paolucci, Pier Stanislao},   
	 
TITLE={Fast Simulations of Highly-Connected Spiking Cortical Models Using GPUs},      
	
JOURNAL={Frontiers in Computational Neuroscience},      
	
VOLUME={15},           
	
YEAR={2021},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fncom.2021.627620},       
	
DOI={10.3389/fncom.2021.627620},      
	
ISSN={1662-5188},   
   
ABSTRACT={Over the past decade there has been a growing interest in the development of parallel hardware systems for simulating large-scale networks of spiking neurons. Compared to other highly-parallel systems, GPU-accelerated solutions have the advantage of a relatively low cost and a great versatility, thanks also to the possibility of using the CUDA-C/C++ programming languages. NeuronGPU is a GPU library for large-scale simulations of spiking neural network models, written in the C++ and CUDA-C++ programming languages, based on a novel spike-delivery algorithm. This library includes simple LIF (leaky-integrate-and-fire) neuron models as well as several multisynapse AdEx (adaptive-exponential-integrate-and-fire) neuron models with current or conductance based synapses, different types of spike generators, tools for recording spikes, state variables and parameters, and it supports user-definable models. The numerical solution of the differential equations of the dynamics of the AdEx models is performed through a parallel implementation, written in CUDA-C++, of the fifth-order Runge-Kutta method with adaptive step-size control. In this work we evaluate the performance of this library on the simulation of a cortical microcircuit model, based on LIF neurons and current-based synapses, and on balanced networks of excitatory and inhibitory neurons, using AdEx or Izhikevich neuron models and conductance-based or current-based synapses. On these models, we will show that the proposed library achieves state-of-the-art performance in terms of simulation time per second of biological activity. In particular, using a single NVIDIA GeForce RTX 2080 Ti GPU board, the full-scale cortical-microcircuit model, which includes about 77,000 neurons and 3 · 10<sup>8</sup> connections, can be simulated at a speed very close to real time, while the simulation time of a balanced network of 1,000,000 AdEx neurons with 1,000 connections per neuron was about 70 s per second of biological activity.}
}

@article{Brette2005,
author = {Brette, Romain and Gerstner, Wulfram},
title = {Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity},
journal = {Journal of Neurophysiology},
volume = {94},
number = {5},
pages = {3637-3642},
year = {2005},
doi = {10.1152/jn.00686.2005},
note ={PMID: 16014787},
URL = {https://doi.org/10.1152/jn.00686.2005},
eprint = {https://doi.org/10.1152/jn.00686.2005},
abstract = { We introduce a two-dimensional integrate-and-fire model that combines an exponential spike mechanism with an adaptation equation, based on recent theoretical findings. We describe a systematic method to estimate its parameters with simple electrophysiological protocols (current-clamp injection of pulses and ramps) and apply it to a detailed conductance-based model of a regular spiking neuron. Our simple model predicts correctly the timing of 96\% of the spikes (±2 ms) of the detailed model in response to injection of noisy synaptic conductances. The model is especially reliable in high-conductance states, typical of cortical activity in vivo, in which intrinsic conductances were found to have a reduced role in shaping spike trains. These results are promising because this simple model has enough expressive power to reproduce qualitatively several electrophysiological classes described in vitro.}
}

@article{Potjans2012,
    author = {Potjans, Tobias C. and Diesmann, Markus},
    title = "{The Cell-Type Specific Cortical Microcircuit: Relating Structure and Activity in a Full-Scale Spiking Network Model}",
    journal = {Cerebral Cortex},
    volume = {24},
    number = {3},
    pages = {785-806},
    year = {2012},
    month = {12},
    abstract = "{In the past decade, the cell-type specific connectivity and activity of local cortical networks have been characterized experimentally to some detail. In parallel, modeling has been established as a tool to relate network structure to activity dynamics. While available comprehensive connectivity maps ( Thomson, West, et al. 2002; Binzegger et al. 2004) have been used in various computational studies, prominent features of the simulated activity such as the spontaneous firing rates do not match the experimental findings. Here, we analyze the properties of these maps to compile an integrated connectivity map, which additionally incorporates insights on the specific selection of target types. Based on this integrated map, we build a full-scale spiking network model of the local cortical microcircuit. The simulated spontaneous activity is asynchronous irregular and cell-type specific firing rates are in agreement with in vivo recordings in awake animals, including the low rate of layer 2/3 excitatory cells. The interplay of excitation and inhibition captures the flow of activity through cortical layers after transient thalamic stimulation. In conclusion, the integration of a large body of the available connectivity data enables us to expose the dynamical consequences of the cortical microcircuitry.}",
    issn = {1047-3211},
    doi = {10.1093/cercor/bhs358},
    url = {https://doi.org/10.1093/cercor/bhs358},
    eprint = {https://academic.oup.com/cercor/article-pdf/24/3/785/14099777/bhs358.pdf},
}

@ARTICLE{VanAlbada2018,
AUTHOR={van Albada, Sacha J. and Rowley, Andrew G. and Senk, Johanna and Hopkins, Michael and Schmidt, Maximilian and Stokes, Alan B. and Lester, David R. and Diesmann, Markus and Furber, Steve B.},
TITLE={Performance Comparison of the Digital Neuromorphic Hardware SpiNNaker and the Neural Network Simulation Software NEST for a Full-Scale Cortical Microcircuit Model},
JOURNAL={Frontiers in Neuroscience},
VOLUME={12},
YEAR={2018}, 
URL={https://www.frontiersin.org/articles/10.3389/fnins.2018.00291},
DOI={10.3389/fnins.2018.00291},
ISSN={1662-453X},
ABSTRACT={The digital neuromorphic hardware SpiNNaker has been developed with the aim of enabling large-scale neural network simulations in real time and with low power consumption. Real-time performance is achieved with 1 ms integration time steps, and thus applies to neural networks for which faster time scales of the dynamics can be neglected. By slowing down the simulation, shorter integration time steps and hence faster time scales, which are often biologically relevant, can be incorporated. We here describe the first full-scale simulations of a cortical microcircuit with biological time scales on SpiNNaker. Since about half the synapses onto the neurons arise within the microcircuit, larger cortical circuits have only moderately more synapses per neuron. Therefore, the full-scale microcircuit paves the way for simulating cortical circuits of arbitrary size. With approximately 80, 000 neurons and 0.3 billion synapses, this model is the largest simulated on SpiNNaker to date. The scale-up is enabled by recent developments in the SpiNNaker software stack that allow simulations to be spread across multiple boards. Comparison with simulations using the NEST software on a high-performance cluster shows that both simulators can reach a similar accuracy, despite the fixed-point arithmetic of SpiNNaker, demonstrating the usability of SpiNNaker for computational neuroscience applications with biological time scales and large network size. The runtime and power consumption are also assessed for both simulators on the example of the cortical microcircuit model. To obtain an accuracy similar to that of NEST with 0.1 ms time steps, SpiNNaker requires a slowdown factor of around 20 compared to real time. The runtime for NEST saturates around 3 times real time using hybrid parallelization with MPI and multi-threading. However, achieving this runtime comes at the cost of increased power and energy consumption. The lowest total energy consumption for NEST is reached at around 144 parallel threads and 4.6 times slowdown. At this setting, NEST and SpiNNaker have a comparable energy consumption per synaptic event. Our results widen the application domain of SpiNNaker and help guide its development, showing that further optimizations such as synapse-centric network representation are necessary to enable real-time simulation of large biological neural networks.}
}

@ARTICLE{Dasbach2021,
AUTHOR={Dasbach, Stefan and Tetzlaff, Tom and Diesmann, Markus and Senk, Johanna},
TITLE={Dynamical Characteristics of Recurrent Neuronal Networks Are Robust Against Low Synaptic Weight Resolution},
JOURNAL={Frontiers in Neuroscience},
VOLUME={15},
YEAR={2021},  
URL={https://www.frontiersin.org/articles/10.3389/fnins.2021.757790},
DOI={10.3389/fnins.2021.757790},
ISSN={1662-453X},
ABSTRACT={The representation of the natural-density, heterogeneous connectivity of neuronal network models at relevant spatial scales remains a challenge for Computational Neuroscience and Neuromorphic Computing. In particular, the memory demands imposed by the vast number of synapses in brain-scale network simulations constitute a major obstacle. Limiting the number resolution of synaptic weights appears to be a natural strategy to reduce memory and compute load. In this study, we investigate the effects of a limited synaptic-weight resolution on the dynamics of recurrent spiking neuronal networks resembling local cortical circuits and develop strategies for minimizing deviations from the dynamics of networks with high-resolution synaptic weights. We mimic the effect of a limited synaptic weight resolution by replacing normally distributed synaptic weights with weights drawn from a discrete distribution, and compare the resulting statistics characterizing firing rates, spike-train irregularity, and correlation coefficients with the reference solution. We show that a naive discretization of synaptic weights generally leads to a distortion of the spike-train statistics. If the weights are discretized such that the mean and the variance of the total synaptic input currents are preserved, the firing statistics remain unaffected for the types of networks considered in this study. For networks with sufficiently heterogeneous in-degrees, the firing statistics can be preserved even if all synaptic weights are replaced by the mean of the weight distribution. We conclude that even for simple networks with non-plastic neurons and synapses, a discretization of synaptic weights can lead to substantial deviations in the firing statistics unless the discretization is performed with care and guided by a rigorous validation process. For the network model used in this study, the synaptic weights can be replaced by low-resolution weights without affecting its macroscopic dynamical characteristics, thereby saving substantial amounts of memory.}
}

@article{Nordlie2009,
    doi = {10.1371/journal.pcbi.1000456},
    author = {Nordlie, Eilen AND Gewaltig, Marc-Oliver AND Plesser, Hans Ekkehard},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Towards Reproducible Descriptions of Neuronal Network Models},
    year = {2009},
    month = {08},
    volume = {5},
    url = {https://doi.org/10.1371/journal.pcbi.1000456},
    pages = {1-18},
    abstract = {Progress in science depends on the effective exchange of ideas among scientists. New ideas can be assessed and criticized in a meaningful manner only if they are formulated precisely. This applies to simulation studies as well as to experiments and theories. But after more than 50 years of neuronal network simulations, we still lack a clear and common understanding of the role of computational models in neuroscience as well as established practices for describing network models in publications. This hinders the critical evaluation of network models as well as their re-use. We analyze here 14 research papers proposing neuronal network models of different complexity and find widely varying approaches to model descriptions, with regard to both the means of description and the ordering and placement of material. We further observe great variation in the graphical representation of networks and the notation used in equations. Based on our observations, we propose a good model description practice, composed of guidelines for the organization of publications, a checklist for model descriptions, templates for tables presenting model structure, and guidelines for diagrams of networks. The main purpose of this good practice is to trigger a debate about the communication of neuronal network models in a manner comprehensible to humans, as opposed to machine-readable model description languages. We believe that the good model description practice proposed here, together with a number of other recent initiatives on data-, model-, and software-sharing, may lead to a deeper and more fruitful exchange of ideas among computational neuroscientists in years to come. We further hope that work on standardized ways of describing—and thinking about—complex neuronal networks will lead the scientific community to a clearer understanding of high-level concepts in network dynamics, and will thus lead to deeper insights into the function of the brain.},
    number = {8},
}

@article{Schmidt2018,
    doi = {10.1371/journal.pcbi.1006359},
    author = {Schmidt, Maximilian AND Bakker, Rembrandt AND Shen, Kelly AND Bezgin, Gleb AND Diesmann, Markus AND van Albada, Sacha Jennifer},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {A multi-scale layer-resolved spiking network model of resting-state dynamics in macaque visual cortical areas},
    year = {2018},
    month = {10},
    volume = {14},
    url = {https://doi.org/10.1371/journal.pcbi.1006359},
    pages = {1-38},
    abstract = {Cortical activity has distinct features across scales, from the spiking statistics of individual cells to global resting-state networks. We here describe the first full-density multi-area spiking network model of cortex, using macaque visual cortex as a test system. The model represents each area by a microcircuit with area-specific architecture and features layer- and population-resolved connectivity between areas. Simulations reveal a structured asynchronous irregular ground state. In a metastable regime, the network reproduces spiking statistics from electrophysiological recordings and cortico-cortical interaction patterns in fMRI functional connectivity under resting-state conditions. Stable inter-area propagation is supported by cortico-cortical synapses that are moderately strong onto excitatory neurons and stronger onto inhibitory neurons. Causal interactions depend on both cortical structure and the dynamical state of populations. Activity propagates mainly in the feedback direction, similar to experimental results associated with visual imagery and sleep. The model unifies local and large-scale accounts of cortex, and clarifies how the detailed connectivity of cortex shapes its dynamics on multiple scales. Based on our simulations, we hypothesize that in the spontaneous condition the brain operates in a metastable regime where cortico-cortical projections target excitatory and inhibitory populations in a balanced manner that produces substantial inter-area interactions while maintaining global stability.},
    number = {10},
}

@ARTICLE{Knight2021,
AUTHOR={Knight, James C. and Komissarov, Anton and Nowotny, Thomas},
TITLE={PyGeNN: A Python Library for GPU-Enhanced Neural Networks},
JOURNAL={Frontiers in Neuroinformatics},
VOLUME={15},
YEAR={2021},
URL={https://www.frontiersin.org/articles/10.3389/fninf.2021.659005},
DOI={10.3389/fninf.2021.659005},
ISSN={1662-5196},
ABSTRACT={More than half of the Top 10 supercomputing sites worldwide use GPU accelerators and they are becoming ubiquitous in workstations and edge computing devices. GeNN is a C++ library for generating efficient spiking neural network simulation code for GPUs. However, until now, the full flexibility of GeNN could only be harnessed by writing model descriptions and simulation code in C++. Here we present PyGeNN, a Python package which exposes all of GeNN's functionality to Python with minimal overhead. This provides an alternative, arguably more user-friendly, way of using GeNN and allows modelers to use GeNN within the growing Python-based machine learning and computational neuroscience ecosystems. In addition, we demonstrate that, in both Python and C++ GeNN simulations, the overheads of recording spiking data can strongly affect runtimes and show how a new spike recording system can reduce these overheads by up to 10×. Using the new recording system, we demonstrate that by using PyGeNN on a modern GPU, we can simulate a full-scale model of a cortical column faster even than real-time neuromorphic systems. Finally, we show that long simulations of a smaller model with complex stimuli and a custom three-factor learning rule defined in PyGeNN can be simulated almost two orders of magnitude faster than real-time.}
}

@ARTICLE{Knight2018,
AUTHOR={Knight, James C. and Nowotny, Thomas},
TITLE={GPUs Outperform Current HPC and Neuromorphic Solutions in Terms of Speed and Energy When Simulating a Highly-Connected Cortical Model},
JOURNAL={Frontiers in Neuroscience},
VOLUME={12},
YEAR={2018},
URL={https://www.frontiersin.org/articles/10.3389/fnins.2018.00941},
DOI={10.3389/fnins.2018.00941},
ISSN={1662-453X},
ABSTRACT={While neuromorphic systems may be the ultimate platform for deploying spiking neural networks (SNNs), their distributed nature and optimization for specific types of models makes them unwieldy tools for developing them. Instead, SNN models tend to be developed and simulated on computers or clusters of computers with standard von Neumann CPU architectures. Over the last decade, as well as becoming a common fixture in many workstations, NVIDIA GPU accelerators have entered the High Performance Computing field and are now used in 50 \% of the Top 10 super computing sites worldwide. In this paper we use our GeNN code generator to re-implement two neo-cortex-inspired, circuit-scale, point neuron network models on GPU hardware. We verify the correctness of our GPU simulations against prior results obtained with NEST running on traditional HPC hardware and compare the performance with respect to speed and energy consumption against published data from CPU-based HPC and neuromorphic hardware. A full-scale model of a cortical column can be simulated at speeds approaching 0.5× real-time using a single NVIDIA Tesla V100 accelerator—faster than is currently possible using a CPU based cluster or the SpiNNaker neuromorphic system. In addition, we find that, across a range of GPU systems, the energy to solution as well as the energy per synaptic event of the microcircuit simulation is as much as 14× lower than either on SpiNNaker or in CPU-based simulations. Besides performance in terms of speed and energy consumption of the simulation, efficient initialization of models is also a crucial concern, particularly in a research context where repeated runs and parameter-space exploration are required. Therefore, we also introduce in this paper some of the novel parallel initialization methods implemented in the latest version of GeNN and demonstrate how they can enable further speed and energy advantages.}
}

@article{Rhodes2019,
  doi = {10.1098/rsta.2019.0160},
  url = {https://doi.org/10.1098/rsta.2019.0160},
  year = {2019},
  month = dec,
  publisher = {The Royal Society},
  volume = {378},
  number = {2164},
  pages = {20190160},
  author = {Oliver Rhodes and Luca Peres and Andrew G. D. Rowley and Andrew Gait and Luis A. Plana and Christian Brenninkmeijer and Steve B. Furber},
  title = {Real-time cortical simulation on neuromorphic hardware},
  journal = {Philosophical Transactions of the Royal Society A: Mathematical,  Physical and Engineering Sciences}
}

@article{Kurth2022,
doi = {10.1088/2634-4386/ac55fc},
url = {https://dx.doi.org/10.1088/2634-4386/ac55fc},
year = {2022},
month = {mar},
publisher = {IOP Publishing},
volume = {2},
number = {2},
pages = {021001},
author = {Anno C Kurth and Johanna Senk and Dennis Terhorst and Justin Finnerty and Markus Diesmann},
title = {Sub-realtime simulation of a neuronal network of natural density},
journal = {Neuromorphic Computing and Engineering },
abstract = {Full scale simulations of neuronal network models of the brain are challenging due to the high density of connections between neurons. This contribution reports run times shorter than the simulated span of biological time for a full scale model of the local cortical microcircuit with explicit representation of synapses on a recent conventional compute node. Realtime performance is relevant for robotics and closed-loop applications while sub-realtime is desirable for the study of learning and development in the brain, processes extending over hours and days of biological time.}
}

@ARTICLE{Heittmann2022,
AUTHOR={Heittmann, Arne and Psychou, Georgia and Trensch, Guido and Cox, Charles E. and Wilcke, Winfried W. and Diesmann, Markus and Noll, Tobias G.},
TITLE={Simulating the Cortical Microcircuit Significantly Faster Than Real Time on the IBM INC-3000 Neural Supercomputer},
JOURNAL={Frontiers in Neuroscience},
VOLUME={15},
YEAR={2022},
URL={https://www.frontiersin.org/articles/10.3389/fnins.2021.728460},
DOI={10.3389/fnins.2021.728460},
ISSN={1662-453X},
ABSTRACT={This article employs the new IBM INC-3000 prototype FPGA-based neural supercomputer to implement a widely used model of the cortical microcircuit. With approximately 80,000 neurons and 300 Million synapses this model has become a benchmark network for comparing simulation architectures with regard to performance. To the best of our knowledge, the achieved speed-up factor is 2.4 times larger than the highest speed-up factor reported in the literature and four times larger than biological real time demonstrating the potential of FPGA systems for neural modeling. The work was performed at Jülich Research Centre in Germany and the INC-3000 was built at the IBM Almaden Research Center in San Jose, CA, United States. For the simulation of the microcircuit only the programmable logic part of the FPGA nodes are used. All arithmetic is implemented with single-floating point precision. The original microcircuit network with linear LIF neurons and current-based exponential-decay-, alpha-function- as well as beta-function-shaped synapses was simulated using exact exponential integration as ODE solver method. In order to demonstrate the flexibility of the approach, additionally networks with non-linear neuron models (AdEx, Izhikevich) and conductance-based synapses were simulated, applying Runge–Kutta and Parker–Sochacki solver methods. In all cases, the simulation-time speed-up factor did not decrease by more than a very few percent. It finally turns out that the speed-up factor is essentially limited by the latency of the INC-3000 communication system.}
}

@Article{Golosio2023,
AUTHOR = {Golosio, Bruno and Villamar, Jose and Tiddia, Gianmarco and Pastorelli, Elena and Stapmanns, Jonas and Fanti, Viviana and Paolucci, Pier Stanislao and Morrison, Abigail and Senk, Johanna},
TITLE = {Runtime Construction of Large-Scale Spiking Neuronal Network Models on GPU Devices},
JOURNAL = {Applied Sciences},
VOLUME = {13},
YEAR = {2023},
NUMBER = {17},
ARTICLE-NUMBER = {9598},
URL = {https://www.mdpi.com/2076-3417/13/17/9598},
ISSN = {2076-3417},
ABSTRACT = {Simulation speed matters for neuroscientific research: this includes not only how quickly the simulated model time of a large-scale spiking neuronal network progresses but also how long it takes to instantiate the network model in computer memory. On the hardware side, acceleration via highly parallel GPUs is being increasingly utilized. On the software side, code generation approaches ensure highly optimized code at the expense of repeated code regeneration and recompilation after modifications to the network model. Aiming for a greater flexibility with respect to iterative model changes, here we propose a new method for creating network connections interactively, dynamically, and directly in GPU memory through a set of commonly used high-level connection rules. We validate the simulation performance with both consumer and data center GPUs on two neuroscientifically relevant models: a cortical microcircuit of about 77,000 leaky-integrate-and-fire neuron models and 300 million static synapses, and a two-population network recurrently connected using a variety of connection rules. With our proposed ad hoc network instantiation, both network construction and simulation times are comparable or shorter than those obtained with other state-of-the-art simulation technologies while still meeting the flexibility demands of explorative network modeling.},
DOI = {10.3390/app13179598}
}

@article{Schmidt2018b,
	title = {Multi-scale account of the network structure of macaque visual cortex},
	volume = {223},
	issn = {1863-2661},
	doi = {10.1007/s00429-017-1554-4},
	number = {3},
	journal = {Brain Structure and Function},
	author = {Schmidt, Maximilian and Bakker, Rembrandt and Hilgetag, Claus C. and Diesmann, Markus and van Albada, Sacha J.},
	month = apr,
	year = {2018},
	pmid = {29143946},
	pmcid = {PMC5869897},
	keywords = {Cellular architecture, Cortical layers, Macaque visual cortex, Multi-scale connectivity, Predictive connectomics},
	pages = {1409--1435},
        url = {https://doi.org/10.1007/s00429-017-1554-4},
}

@ARTICLE{Tiddia2022,
AUTHOR={Tiddia, Gianmarco and Golosio, Bruno and Albers, Jasper and Senk, Johanna and Simula, Francesco and Pronold, Jari and Fanti, Viviana and Pastorelli, Elena and Paolucci, Pier Stanislao and van Albada, Sacha J.},
TITLE={Fast Simulation of a Multi-Area Spiking Network Model of Macaque Cortex on an MPI-GPU Cluster},
JOURNAL={Frontiers in Neuroinformatics},
VOLUME={16},
YEAR={2022},
URL={https://www.frontiersin.org/articles/10.3389/fninf.2022.883333},
DOI={10.3389/fninf.2022.883333},
ISSN={1662-5196},
ABSTRACT={Spiking neural network models are increasingly establishing themselves as an effective tool for simulating the dynamics of neuronal populations and for understanding the relationship between these dynamics and brain function. Furthermore, the continuous development of parallel computing technologies and the growing availability of computational resources are leading to an era of large-scale simulations capable of describing regions of the brain of ever larger dimensions at increasing detail. Recently, the possibility to use MPI-based parallel codes on GPU-equipped clusters to run such complex simulations has emerged, opening up novel paths to further speed-ups. NEST GPU is a GPU library written in CUDA-C/C++ for large-scale simulations of spiking neural networks, which was recently extended with a novel algorithm for remote spike communication through MPI on a GPU cluster. In this work we evaluate its performance on the simulation of a multi-area model of macaque vision-related cortex, made up of about 4 million neurons and 24 billion synapses and representing 32mm<sup>2</sup> surface area of the macaque cortex. The outcome of the simulations is compared against that obtained using the well-known CPU-based spiking neural network simulator NEST on a high-performance computing cluster. The results show not only an optimal match with the NEST statistical measures of the neural activity in terms of three informative distributions, but also remarkable achievements in terms of simulation time per second of biological activity. Indeed, NEST GPU was able to simulate a second of biological time of the full-scale macaque cortex model in its metastable state 3.1× faster than NEST using 32 compute nodes equipped with an NVIDIA V100 GPU each. Using the same configuration, the ground state of the full-scale macaque cortex model was simulated 2.4× faster than NEST.}
}

@article{Felleman1991,
  author =       {Felleman, Daniel J and {Van Essen}, David C},
  dateadded =    {2010-02-23},
  journal =      CerebCortex,
  lastdatemodified ={2010-02-23},
  lastname =     {Felleman},
  own =          {notown},
  pages =        {1--47},
  read =         {notread},
  title =        {Distributed hierarchical processing in the primate cerebral cortex},
  volume =       1,
  year =         1991
}

@article{Bakker2012,
  author =       {Bakker, Rembrandt and Thomas, Wachtler and Diesmann, Markus},
  doi =          {10.3389/fninf.2012.00030},
  journal =      {Frontiers in Neuroinformatics},
  pages =       30,
  title =        {{CoCoMac} 2.0 and the future of tract-tracing databases.},
  volume =       6,
  year =         2012
}

@article{Markov2011,
  title={Weight consistency specifies regularities of macaque cortical networks},
  author={Markov, Nikola T and Misery, P and Falchier, Arnaud and Lamy, C and Vezoli, J and Quilodran, R and Gariel, MA and Giroud, Pascale and Ercsey-Ravasz, Maria and Pilaz, LJ and others},
  journal={Cerebral Cortex},
  volume={21},
  number={6},
  pages={1254--1272},
  year={2011},
  publisher={Oxford University Press}
}

@article{Markov2014,
  title={A weighted and directed interareal connectivity matrix for macaque cerebral cortex},
  author={Markov, Nikola T and Ercsey-Ravasz, Maria M and Ribeiro Gomes, AR and Lamy, Camille and Magrou, Loic and Vezoli, Julien and Misery, Pierre and Falchier, Arnaud and Quilodran, Rene and Gariel, Marie-Alice and others},
  journal={Cerebral Cortex},
  volume={24},
  number={1},
  pages={17--36},
  year={2014},
  publisher={Oxford University Press}
}

@article{Schuecker2017,
  doi = {10.1371/journal.pcbi.1005179},
  url = {https://doi.org/10.1371/journal.pcbi.1005179},
  year = {2017},
  month = feb,
  publisher = {Public Library of Science ({PLoS})},
  volume = {13},
  number = {2},
  pages = {e1005179},
  author = {Jannis Schuecker and Maximilian Schmidt and Sacha J. van Albada and Markus Diesmann and Moritz Helias},
  editor = {J\"{o}rn Diedrichsen},
  title = {Fundamental Activity Constraints Lead to Specific Interpretations of the Connectome},
  journal = {{PLOS} Computational Biology}
}

@conference{elephant2018,
    author = {Denker, M. and Yegenoglu, A. and Grün, S.},
    booktitle = {Neuroinformatics 2018},
    title = {{C}ollaborative {HPC}-enabled workflows on the {HBP} {C}ollaboratory using the {E}lephant framework},
    pages = {P19},
    year = {2018},
    doi = {10.12751/incf.ni2018.0019},
    url = {https://abstracts.g-node.org/conference/NI2018/abstracts#/uuid/023bec4e-0c35-4563-81ce-2c6fac282abd},
}

@article{Rosenblatt1956,
  doi = {10.1214/aoms/1177728190},
  url = {https://doi.org/10.1214/aoms/1177728190},
  year = {1956},
  month = sep,
  publisher = {Institute of Mathematical Statistics},
  volume = {27},
  number = {3},
  pages = {832--837},
  author = {Murray Rosenblatt},
  title = {Remarks on Some Nonparametric Estimates of a Density Function},
  journal = {The Annals of Mathematical Statistics}
}

@article{Parzen1962,
  doi = {10.1214/aoms/1177704472},
  url = {https://doi.org/10.1214/aoms/1177704472},
  year = {1962},
  month = sep,
  publisher = {Institute of Mathematical Statistics},
  volume = {33},
  number = {3},
  pages = {1065--1076},
  author = {Emanuel Parzen},
  title = {On Estimation of a Probability Density Function and Mode},
  journal = {The Annals of Mathematical Statistics}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@book{Silverman1986,
  added-at = {2011-05-09T23:10:52.000+0200},
  address = {London},
  author = {Silverman, B. W.},
  biburl = {https://www.bibsonomy.org/bibtex/209d072207f82bbc4d702174f670a2f02/josephausterwei},
  interhash = {caf150ab6a2f46dfe3f0e74864480d03},
  intrahash = {09d072207f82bbc4d702174f670a2f02},
  keywords = {imported},
  publisher = {Chapman and Hall},
  timestamp = {2011-05-10T10:42:42.000+0200},
  title = {Density estimation for statistics and data analysis},
  year = 1986
}

@misc{kde_wiki,
author = "{Wikipedia contributors}",
title = "Kernel density estimation --- {Wikipedia}{,} The Free Encyclopedia",
year = "2023",
howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Kernel_density_estimation&oldid=1158176361}",
note = "[Online; accessed 25-June-2023]"
}

@article{Panaretos_Zemel,
author = {Panaretos, Victor M. and Zemel, Yoav},
title = {Statistical Aspects of {W}asserstein Distances},
journal = {Annual Review of Statistics and its Application},
volume = {6},
number = {1},
pages = {405-431},
year = {2019},
doi = {10.1146/annurev-statistics-030718-104938},
URL = {https://doi.org/10.1146/annurev-statistics-030718-104938},
eprint = {https://doi.org/10.1146/annurev-statistics-030718-104938},
abstract = { Wasserstein distances are metrics on probability distributions inspired by the problem of optimal mass transportation. Roughly speaking, they measure the minimal effort required to reconfigure the probability mass of one distribution in order to recover the other distribution. They are ubiquitous in mathematics, with a long history that has seen them catalyze core developments in analysis, optimization, and probability. Beyond their intrinsic mathematical richness, they possess attractive features that make them a versatile tool for the statistician: They can be used to derive weak convergence and convergence of moments, and can be easily bounded; they are well-adapted to quantify a natural notion of perturbation of a probability distribution; and they seamlessly incorporate the geometry of the domain of the distributions in question, thus being useful for contrasting complex objects. Consequently, they frequently appear in the development of statistical theory and inferential methodology, and they have recently become an object of inference in themselves. In this review, we provide a snapshot of the main concepts involved in Wasserstein distances and optimal transportation, and a succinct overview of some of their many statistical aspects. }
}

@article{Frohmader_Volkmer,
   title={1-{W}asserstein distance on the standard simplex},
   volume={12},
   ISSN={2693-2997},
   url={http://dx.doi.org/10.2140/astat.2021.12.43},
   DOI={10.2140/astat.2021.12.43},
   number={1},
   journal={Algebraic Statistics},
   publisher={Mathematical Sciences Publishers},
   author={Frohmader, Andrew and Volkmer, Hans},
   year={2021},
   month={Apr},
   pages={43–56}
}

@article{Ramdas_Trillos_Cuturi,
AUTHOR = {Ramdas, Aaditya and Trillos, Nicolás García and Cuturi, Marco},
TITLE = {On {W}asserstein Two-Sample Testing and Related Families of Nonparametric Tests},
JOURNAL = {Entropy},
VOLUME = {19},
YEAR = {2017},
NUMBER = {2},
ARTICLE-NUMBER = {47},
URL = {https://www.mdpi.com/1099-4300/19/2/47},
ISSN = {1099-4300},
ABSTRACT = {Nonparametric two-sample or homogeneity testing is a decision theoretic problem that involves identifying differences between two random variables without making parametric assumptions about their underlying distributions. The literature is old and rich, with a wide variety of statistics having being designed and analyzed, both for the unidimensional and the multivariate setting. In this short survey, we focus on test statistics that involve the Wasserstein distance. Using an entropic smoothing of the Wasserstein distance, we connect these to very different tests including multivariate methods involving energy statistics and kernel based maximum mean discrepancy and univariate methods like the Kolmogorov–Smirnov test, probability or quantile (PP/QQ) plots and receiver operating characteristic or ordinal dominance (ROC/ODC) curves. Some observations are implicit in the literature, while others seem to have not been noticed thus far. Given nonparametric two-sample testing’s classical and continued importance, we aim to provide useful connections for theorists and practitioners familiar with one subset of methods but not others.},
DOI = {10.3390/e19020047}
}

@article{Vallender,
author = {Vallender, S. S.},
title = {Calculation of the {W}asserstein Distance Between Probability Distributions on the Line},
journal = {Theory of Probability \& Its Applications},
volume = {18},
number = {4},
pages = {784-786},
year = {1974},
doi = {10.1137/1118101},
URL = {https://doi.org/10.1137/1118101},
eprint = {https://doi.org/10.1137/1118101}
}

@ARTICLE{SciPy,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

@article{Kullback1951,
author = {S. Kullback and R. A. Leibler},
title = {{On Information and Sufficiency}},
volume = {22},
journal = {The Annals of Mathematical Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {79 -- 86},
year = {1951},
doi = {10.1214/aoms/1177729694},
URL = {https://doi.org/10.1214/aoms/1177729694}
}

@article{Gewaltig2007,
AUTHOR = {Gewaltig, M.  and Diesmann, M. },
TITLE   = {{N}{E}{S}{T} ({N}{E}ural {S}imulation {T}ool)},
YEAR    = {2007},
JOURNAL = {Scholarpedia},
VOLUME  = {2},
NUMBER  = {4},
PAGES   = {1430},
DOI     = {10.4249/scholarpedia.1430},
NOTE    = {revision \#130182}
}

@book{Carnevale_2006,
	doi = {10.1017/cbo9780511541612},
	url = {https://doi.org/10.1017%2Fcbo9780511541612},
	year = 2006,
	publisher = {Cambridge University Press},
	author = {Nicholas T. Carnevale and Michael L. Hines},
	title = {The {NEURON} Book}
}

@article{Stimberg2019,
    title = {Brian 2, an intuitive and efficient neural simulator},
    volume = {8},
    issn = {2050-084X},
    doi = {10.7554/eLife.47314},
    journal = {eLife},
    author = {Stimberg, Marcel and Brette, Romain and Goodman, Dan FM},
    editor = {Skinner, Frances K},
    month = aug,
    year = {2019},
    pages = {e47314}
}

@article{Vitay2015,
  doi = {10.3389/fninf.2015.00019},
  url = {https://doi.org/10.3389/fninf.2015.00019},
  year = {2015},
  month = jul,
  publisher = {Frontiers Media {SA}},
  volume = {9},
  author = {Julien Vitay and Helge \"{U}. Dinkelbach and Fred H. Hamker},
  title = {{ANNarchy}: a code generation approach to neural simulations on parallel hardware},
  journal = {Frontiers in Neuroinformatics}
}

@article{Yavuz2016,
  doi = {10.1038/srep18854},
  url = {https://doi.org/10.1038/srep18854},
  year = {2016},
  month = jan,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {6},
  number = {1},
  author = {Esin Yavuz and James Turner and Thomas Nowotny},
  title = {{GeNN}: a code generation framework for accelerated brain simulations},
  journal = {Scientific Reports}
}

@INPROCEEDINGS{CARLsim6,
  author={Niedermeier, Lars and Chen, Kexin and Xing, Jinwei and Das, Anup and Kopsick, Jeffrey and Scott, Eric and Sutton, Nate and Weber, Killian and Dutt, Nikil and Krichmar, Jeffrey L.},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)}, 
  title={CARLsim 6: An Open Source Library for Large-Scale, Biologically Detailed Spiking Neural Network Simulation}, 
  year={2022},
  volume={},
  number={},
  pages={1-10},
  doi={10.1109/IJCNN55064.2022.9892644}}
  
@article{Stimberg2020,
  doi = {10.1038/s41598-019-54957-7},
  url = {https://doi.org/10.1038/s41598-019-54957-7},
  year = {2020},
  month = jan,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {10},
  number = {1},
  author = {Marcel Stimberg and Dan F. M. Goodman and Thomas Nowotny},
  title = {Brian2GeNN: accelerating spiking neural network simulations with graphics hardware},
  journal = {Scientific Reports}
}

@ARTICLE{Kumbhar2019,
AUTHOR={Kumbhar, Pramod and Hines, Michael and Fouriaux, Jeremy and Ovcharenko, Aleksandr and King, James and Delalondre, Fabien and Schürmann, Felix},   
TITLE={CoreNEURON : An Optimized Compute Engine for the NEURON Simulator},      
JOURNAL={Frontiers in Neuroinformatics},      
VOLUME={13},           
YEAR={2019},      
URL={https://www.frontiersin.org/articles/10.3389/fninf.2019.00063},       
DOI={10.3389/fninf.2019.00063},      
ISSN={1662-5196},   
ABSTRACT={The NEURON simulator has been developed over the past three decades and is widely used by neuroscientists to model the electrical activity of neuronal networks. Large network simulation projects using NEURON have supercomputer allocations that individually measure in the millions of core hours. Supercomputer centers are transitioning to next generation architectures and the work accomplished per core hour for these simulations could be improved by an order of magnitude if NEURON was able to better utilize those new hardware capabilities. In order to adapt NEURON to evolving computer architectures, the compute engine of the NEURON simulator has been extracted and has been optimized as a library called CoreNEURON. This paper presents the design, implementation, and optimizations of CoreNEURON. We describe how CoreNEURON can be used as a library with NEURON and then compare performance of different network models on multiple architectures including IBM BlueGene/Q, Intel Skylake, Intel MIC and NVIDIA GPU. We show how CoreNEURON can simulate existing NEURON network models with 4–7x less memory usage and 2–7x less execution time while maintaining binary result compatibility with NEURON.}
}

@ARTICLE{TrueNorth2015,
  author={Akopyan, Filipp and Sawada, Jun and Cassidy, Andrew and Alvarez-Icaza, Rodrigo and Arthur, John and Merolla, Paul and Imam, Nabil and Nakamura, Yutaka and Datta, Pallab and Nam, Gi-Joon and Taba, Brian and Beakes, Michael and Brezzo, Bernard and Kuang, Jente B. and Manohar, Rajit and Risk, William P. and Jackson, Bryan and Modha, Dharmendra S.},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  title={TrueNorth: Design and Tool Flow of a 65 mW 1 Million Neuron Programmable Neurosynaptic Chip},
  year={2015},
  volume={34},
  number={10},
  pages={1537-1557},
  doi={10.1109/TCAD.2015.2474396}
}

@ARTICLE{Loihi2018,
    author={Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and Liao, Yuyun and Lin, Chit-Kwan and Lines, Andrew and Liu, Ruokun and Mathaikutty, Deepak and McCoy, Steven and Paul, Arnab and Tse, Jonathan and Venkataramanan, Guruguhanathan and Weng, Yi-Hsin and Wild, Andreas and Yang, Yoonseok and Wang, Hong},
    journal={IEEE Micro}, 
    title={Loihi: A Neuromorphic Manycore Processor with On-Chip Learning}, 
    year={2018},
    volume={38},
    number={1},
    pages={82-99},
    doi={10.1109/MM.2018.112130359}
}

@article{Grbl2020,
	doi = {10.1007/s11265-020-01558-7},
	url = {https://doi.org/10.1007%2Fs11265-020-01558-7},
	year = 2020,
	month = {jul},
	publisher = {Springer Science and Business Media {LLC}},
	volume = {92},
	number = {11},
	pages = {1277--1292},
	author = {Andreas Grübl and Sebastian Billaudelle and Benjamin Cramer and Vitali Karasenko and Johannes Schemmel},
	title = {Verification and Design Methods for the {BrainScaleS} Neuromorphic Hardware System},
	journal = {Journal of Signal Processing Systems}
}

@ARTICLE{Furber2014,  
author={Furber, Steve B. and Galluppi, Francesco and Temple, Steve and Plana, Luis A.}, 
journal={Proceedings of the IEEE},
title={The {SpiNNaker} Project},
year={2014},
volume={102},
number={5},
pages={652-665},
doi={10.1109/JPROC.2014.2304638}}
  
@article{Rhodes2020,
author = {Rhodes, Oliver  and Peres, Luca  and Rowley, Andrew G. D.  and Gait, Andrew  and Plana, Luis A.  and Brenninkmeijer, Christian  and Furber, Steve B. },
title = {Real-time cortical simulation on neuromorphic hardware},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
volume = {378},
number = {2164},
pages = {20190160},
year = {2020},
doi = {10.1098/rsta.2019.0160},
URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2019.0160},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2019.0160},
abstract = { Real-time simulation of a large-scale biologically representative spiking neural network is presented, through the use of a heterogeneous parallelization scheme and SpiNNaker neuromorphic hardware. A published cortical microcircuit model is used as a benchmark test case, representing ≈1 mm2 of early sensory cortex, containing 77 k neurons and 0.3 billion synapses. This is the first hard real-time simulation of this model, with 10 s of biological simulation time executed in 10 s wall-clock time. This surpasses best-published efforts on HPC neural simulators (3 × slowdown) and GPUs running optimized spiking neural network (SNN) libraries (2 × slowdown). Furthermore, the presented approach indicates that real-time processing can be maintained with increasing SNN size, breaking the communication barrier incurred by traditional computing machinery. Model results are compared to an established HPC simulator baseline to verify simulation correctness, comparing well across a range of statistical measures. Energy to solution and energy per synaptic event are also reported, demonstrating that the relatively low-tech SpiNNaker processors achieve a 10 × reduction in energy relative to modern HPC systems, and comparable energy consumption to modern GPUs. Finally, system robustness is demonstrated through multiple 12 h simulations of the cortical microcircuit, each simulating 12 h of biological time, and demonstrating the potential of neuromorphic hardware as a neuroscience research tool for studying complex spiking neural networks over extended time periods. This article is part of the theme issue ‘Harmonizing energy-autonomous computing and intelligence’. }
}
  
@article{Kauth2023,
  doi = {10.3389/fncom.2023.1144143},
  url = {https://doi.org/10.3389/fncom.2023.1144143},
  year = {2023},
  month = apr,
  publisher = {Frontiers Media {SA}},
  volume = {17},
  author = {Kevin Kauth and Tim Stadtmann and Vida Sobhani and Tobias Gemmeke},
  title = {{neuroAIx}-Framework: design of future neuroscience simulation systems exhibiting execution of the cortical microcircuit model 20{\texttimes} faster than biological real-time},
  journal = {Frontiers in Computational Neuroscience}
}
  
@ARTICLE{Wunderlich2019,
AUTHOR={Wunderlich, Timo and Kungl, Akos F. and Müller, Eric and Hartel, Andreas and Stradmann, Yannik and Aamir, Syed Ahmed and Grübl, Andreas and Heimbrecht, Arthur and Schreiber, Korbinian and Stöckel, David and Pehle, Christian and Billaudelle, Sebastian and Kiene, Gerd and Mauch, Christian and Schemmel, Johannes and Meier, Karlheinz and Petrovici, Mihai A.}, 
TITLE={Demonstrating Advantages of Neuromorphic Computation: A Pilot Study},
JOURNAL={Frontiers in Neuroscience},
VOLUME={13},
YEAR={2019},
URL={https://www.frontiersin.org/article/10.3389/fnins.2019.00260},
DOI={10.3389/fnins.2019.00260},
ISSN={1662-453X},
ABSTRACT={Neuromorphic devices represent an attempt to mimic aspects of the brain's architecture and dynamics with the aim of replicating its hallmark functional capabilities in terms of computational power, robust learning and energy efficiency. We employ a single-chip prototype of the BrainScaleS 2 neuromorphic system to implement a proof-of-concept demonstration of reward-modulated spike-timing-dependent plasticity in a spiking network that learns to play a simplified version of the Pong video game by smooth pursuit. This system combines an electronic mixed-signal substrate for emulating neuron and synapse dynamics with an embedded digital processor for on-chip learning, which in this work also serves to simulate the virtual environment and learning agent. The analog emulation of neuronal membrane dynamics enables a 1000-fold acceleration with respect to biological real-time, with the entire chip operating on a power budget of 57 mW. Compared to an equivalent simulation using state-of-the-art software, the on-chip emulation is at least one order of magnitude faster and three orders of magnitude more energy-efficient. We demonstrate how on-chip learning can mitigate the effects of fixed-pattern noise, which is unavoidable in analog substrates, while making use of temporal variability for action exploration. Learning compensates imperfections of the physical substrate, as manifested in neuronal parameter variability, by adapting synaptic weights to match respective excitability of individual neurons.}
}
  
@article{Guttler2017,
  doi = {10.11588/HEIDOK.00023723},
  url = {http://archiv.ub.uni-heidelberg.de/volltextserver/id/eprint/23723},
  author = {G\"{u}ttler,  Gilbert Maurice},
  keywords = {530 Physics},
  title = {Achieving a Higher Integration Level of Neuromorphic Hardware using Wafer Embedding},
  publisher = {Heidelberg University Library},
  year = {2017}
}

@article{Bekolay2014,
  title = {Nengo: a {Python} tool for building large-scale functional brain models},
  author = {Bekolay, Trevor and Bergstra, James and Hunsberger, Eric
            and DeWolf, Travis and Stewart, Terrence and Rasmussen, Daniel
            and Choo, Xuan and Voelker, Aaron and Eliasmith, Chris},
  journal = {Frontiers in Neuroinformatics},
  pages = {1--13},
  volume = {7},
  number = {48},
  year = {2014},
  issn = {1662-5196},
  doi = {10.3389/fninf.2013.00048}
}
  
@article{Alevi2022,
  doi = {10.3389/fninf.2022.883700},
  url = {https://doi.org/10.3389/fninf.2022.883700},
  year = {2022},
  month = oct,
  publisher = {Frontiers Media {SA}},
  volume = {16},
  author = {Denis Alevi and Marcel Stimberg and Henning Sprekeler and Klaus Obermayer and Moritz Augustin},
  title = {Brian2CUDA: Flexible and Efficient Simulation of Spiking Neural Network Models on {GPUs}},
  journal = {Frontiers in Neuroinformatics}
}
  
@INPROCEEDINGS{paper:arbor2019,
    author={N. {Abi Akar} and B. {Cumming} and V. {Karakasis} and A. {Küsters} and W. {Klijn} and A. {Peyser} and S. {Yates}},
    booktitle={2019 27th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)},
    title={{Arbor --- A Morphologically-Detailed Neural Network Simulation Library for Contemporary High-Performance Computing Architectures}},
    year={2019}, month={feb}, volume={}, number={},
    pages={274--282},
    doi={10.1109/EMPDP.2019.8671560},
    ISSN={2377-5750}
}

@book{sanders10,
  abstract = {CUDA is a computing architecture designed to facilitate the development of parallel programs. This book shows programmers how to employ this new technology. Each area of CUDA development is introduced through working examples. After a concise introduction to the CUDA platform and architecture, as well as a quick-start guide to CUDA C, the book details the techniques and trade-offs associated with each key CUDA feature.--[book cover].},
  added-at = {2016-02-09T14:59:40.000+0100},
  address = {Upper Saddle River, NJ},
  author = {Sanders, Jason and Kandrot, Edward},
  biburl = {https://www.bibsonomy.org/bibtex/27431903b5946bbf782f30227f3107349/ytyoun},
  interhash = {b2b0b8345185c46225a80577b40f58e1},
  intrahash = {7431903b5946bbf782f30227f3107349},
  isbn = {9780131387683 0131387685},
  keywords = {cuda gpu parallel programming textbook},
  publisher = {Addison-Wesley},
  refid = {535495666},
  timestamp = {2016-02-09T14:59:40.000+0100},
  title = {{CUDA} by Example: An Introduction to General-Purpose {GPU} Programming},
  year = 2010
}

@article{Knight2021b,
  doi = {10.1038/s43588-020-00022-7},
  url = {https://doi.org/10.1038/s43588-020-00022-7},
  year = {2021},
  month = feb,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {1},
  number = {2},
  pages = {136--142},
  author = {James C. Knight and Thomas Nowotny},
  title = {Larger {GPU}-accelerated brain simulations with procedural connectivity},
  journal = {Nature Computational Science}
}


@ARTICLE{Pastorelli2019,
  AUTHOR={Pastorelli, Elena and Capone, Cristiano and Simula, Francesco and Sanchez-Vives, Maria V. and Del Giudice, Paolo and Mattia, Maurizio and Paolucci, Pier Stanislao},   
  TITLE={Scaling of a Large-Scale Simulation of Synchronous Slow-Wave and Asynchronous Awake-Like Activity of a Cortical Model With Long-Range Interconnections},      
  JOURNAL={Frontiers in Systems Neuroscience},      
  VOLUME={13},      
  PAGES={33},     
  YEAR={2019},  
  URL={https://www.frontiersin.org/article/10.3389/fnsys.2019.00033},       
  DOI={10.3389/fnsys.2019.00033},      
  ISSN={1662-5137},   
  ABSTRACT={Cortical synapse organization supports a range of dynamic states on multiple spatial and temporal scales, from synchronous slow wave activity (SWA), characteristic of deep sleep or anesthesia, to fluctuating, asynchronous activity during wakefulness (AW). Such dynamic diversity poses a challenge for producing efficient large-scale simulations that embody realistic metaphors of short- and long-range synaptic connectivity. In fact, during SWA and AW different spatial extents of the cortical tissue are active in a given timespan and at different firing rates, which implies a wide variety of loads of local computation and communication. A balanced evaluation of simulation performance and robustness should therefore include tests of a variety of cortical dynamic states. Here, we demonstrate performance scaling of our proprietary Distributed and Plastic Spiking Neural Networks (DPSNN) simulation engine in both SWA and AW for bidimensional grids of neural populations, which reflects the modular organization of the cortex. We explored networks up to 192 × 192 modules, each composed of 1,250 integrate-and-fire neurons with spike-frequency adaptation, and exponentially decaying inter-modular synaptic connectivity with varying spatial decay constant. For the largest networks the total number of synapses was over 70 billion. The execution platform included up to 64 dual-socket nodes, each socket mounting 8 Intel Xeon Haswell processor cores @ 2.40 GHz clock rate. Network initialization time, memory usage, and execution time showed good scaling performances from 1 to 1,024 processes, implemented using the standard Message Passing Interface (MPI) protocol. We achieved simulation speeds of between 2.3 × 10<sup>9</sup> and 4.1 × 10<sup>9</sup> synaptic events per second for both cortical states in the explored range of inter-modular interconnections.}
}

@inproceedings{Golosio2020,
  title={Toward a possible integration of {NeuronGPU} in {NEST}},
  author={Golosio, Bruno and De Luca, Chiara and Pastorelli, Elena and Simula, Francesco and Tiddia, Gianmarco and Paolucci, Pier Stanislao},
  booktitle={NEST Conference},
  volume={7},
  year={2020}
}

@ARTICLE{Schmitt2023,
AUTHOR={Schmitt, Felix Johannes and Rostami, Vahid and Nawrot, Martin Paul},
TITLE={Efficient parameter calibration and real-time simulation of large-scale spiking neural networks with GeNN and NEST},
JOURNAL={Frontiers in Neuroinformatics},
VOLUME={17},
YEAR={2023},
URL={https://www.frontiersin.org/articles/10.3389/fninf.2023.941696},
DOI={10.3389/fninf.2023.941696},
ISSN={1662-5196},
ABSTRACT={Spiking neural networks (SNNs) represent the state-of-the-art approach to the biologically realistic modeling of nervous system function. The systematic calibration for multiple free model parameters is necessary to achieve robust network function and demands high computing power and large memory resources. Special requirements arise from closed-loop model simulation in virtual environments and from real-time simulation in robotic application. Here, we compare two complementary approaches to efficient large-scale and real-time SNN simulation. The widely used NEural Simulation Tool (NEST) parallelizes simulation across multiple CPU cores. The GPU-enhanced Neural Network (GeNN) simulator uses the highly parallel GPU-based architecture to gain simulation speed. We quantify fixed and variable simulation costs on single machines with different hardware configurations. As a benchmark model, we use a spiking cortical attractor network with a topology of densely connected excitatory and inhibitory neuron clusters with homogeneous or distributed synaptic time constants and in comparison to the random balanced network. We show that simulation time scales linearly with the simulated biological model time and, for large networks, approximately linearly with the model size as dominated by the number of synaptic connections. Additional fixed costs with GeNN are almost independent of model size, while fixed costs with NEST increase linearly with model size. We demonstrate how GeNN can be used for simulating networks with up to 3.5 · 10<sup>6</sup> neurons (> 3 · 10<sup>12</sup>synapses) on a high-end GPU, and up to 250, 000 neurons (25 · 10<sup>9</sup> synapses) on a low-cost GPU. Real-time simulation was achieved for networks with 100, 000 neurons. Network calibration and parameter grid search can be efficiently achieved using batch processing. We discuss the advantages and disadvantages of both approaches for different use cases.}
}

@book{Cormen2009,
author = {Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford},
title = {Introduction to Algorithms, Third Edition},
year = {2009},
isbn = {0262033844},
publisher = {The MIT Press},
edition = {3rd},
abstract = {If you had to buy just one text on algorithms, Introduction to Algorithms is a magnificent choice. The book begins by considering the mathematical foundations of the analysis of algorithms and maintains this mathematical rigor throughout the work. The tools developed in these opening sections are then applied to sorting, data structures, graphs, and a variety of selected algorithms including computational geometry, string algorithms, parallel models of computation, fast Fourier transforms (FFTs), and more. This book's strength lies in its encyclopedic range, clear exposition, and powerful analysis. Pseudo-code explanation of the algorithms coupled with proof of their accuracy makes this book is a great resource on the basic tools used to analyze the performance of algorithms.}
}

@article{seaborn,
  doi = {10.21105/joss.03021},
  url = {https://doi.org/10.21105/joss.03021},
  year = {2021},
  publisher = {The Open Journal},
  volume = {6},
  number = {60},
  pages = {3021},
  author = {Michael L. Waskom},
  title = {seaborn: statistical data visualization},
  journal = {Journal of Open Source Software}
}

@software{nest3.3,
  author       = {Spreizer, Sebastian and
                  Mitchell, Jessica and
                  Jordan, Jakob and
                  Wybo, Willem and
                  Kurth, Anno and
                  Vennemo, Stine Brekke and
                  Pronold, Jari and
                  Trensch, Guido and
                  Benelhedi, Mohamed Ayssar and
                  Terhorst, Dennis and
                  Eppler, Jochen Martin and
                  Mørk, Håkon and
                  Linssen, Charl and
                  Senk, Johanna and
                  Lober, Melissa and
                  Morrison, Abigail and
                  Graber, Steffen and
                  Kunkel, Susanne and
                  Gutzen, Robin and
                  Plesser, Hans Ekkehard},
  title        = {NEST 3.3},
  month        = mar,
  year         = 2022,
  publisher    = {Zenodo},
  version      = {3.3},
  doi          = {10.5281/zenodo.6368024},
  url          = {https://doi.org/10.5281/zenodo.6368024}
}

@software{nest2.20.0,
  author       = {Fardet, Tanguy and
                  Vennemo, Stine Brekke and
                  Mitchell, Jessica and
                  Mørk, Håkon and
                  Graber, Steffen and
                  Hahne, Jan and
                  Spreizer, Sebastian and
                  Deepu, Rajalekshmi and
                  Trensch, Guido and
                  Weidel, Philipp and
                  Jordan, Jakob and
                  Eppler, Jochen Martin and
                  Terhorst, Dennis and
                  Morrison, Abigail and
                  Linssen, Charl and
                  Antonietti, Alberto and
                  Dai, Kael and
                  Serenko, Alexey and
                  Cai, Binghuang and
                  Kubaj, Piotr and
                  Gutzen, Robin and
                  Jiang, Hanjia and
                  Kitayama, Itaru and
                  Jürgens, Björn and
                  Plesser, Hans Ekkehard},
  title        = {NEST 2.20.0},
  month        = jan,
  year         = 2020,
  publisher    = {Zenodo},
  version      = {2.20.0},
  doi          = {10.5281/zenodo.3605514},
  url          = {https://doi.org/10.5281/zenodo.3605514}
}

@article{JUSUF,
  doi = {10.17815/jlsrf-7-179},
  url = {https://doi.org/10.17815/jlsrf-7-179},
  year = {2021},
  month = oct,
  publisher = {Forschungszentrum Julich,  Zentralbibliothek},
  volume = {7},
  author = {Benedikt Von St. Vieth},
  title = {{JUSUF}: Modular Tier-2 Supercomputing and Cloud Infrastructure at J\"{u}lich Supercomputing Centre},
  journal = {Journal of large-scale research facilities {JLSRF}}
}

@article{JURECA,
  doi = {10.17815/jlsrf-7-182},
  url = {https://doi.org/10.17815/jlsrf-7-182},
  year = {2021},
  month = oct,
  publisher = {Forschungszentrum Julich,  Zentralbibliothek},
  volume = {7},
  author = {Philipp Th\"{o}rnig},
  title = {{JURECA}: Data Centric and Booster Modules implementing the Modular Supercomputing Architecture at J\"{u}lich Supercomputing Centre},
  journal = {Journal of large-scale research facilities {JLSRF}}
}

@software{nest2.14.1,
  author       = {Peyser, Alexander and
                  Senk, Johanna and
                  Pronold, Jari and
                  Sinha, Ankur and
                  Vennemo, Stine Brekke and
                  Ippen, Tammo and
                  Jordan, Jakob and
                  Graber, Steffen and
                  Morrison, Abigail and
                  Trensch, Guido and
                  Fardet, Tanguy and
                  Mørk, Håkon and
                  Hahne, Jan and
                  Schuecker, Jannis and
                  Schmidt, Maximilian and
                  Kunkel, Susanne and
                  Dahmen, David and
                  Eppler, Jochen Martin and
                  Diaz, Sandra and
                  Terhorst, Dennis and
                  Deepu, Rajalekshmi and
                  Weidel, Philipp and
                  Kitayama, Itaru and
                  Mahmoudian, Sepehr and
                  Kappel, David and
                  Schulze, Martin and
                  Appukuttan, Shailesh and
                  Schumann, Till and
                  Tunç, Hünkar Can and
                  Mitchell, Jessica and
                  Hoff, Michael and
                  Müller, Eric and
                  Carvalho, Milena Menezes and
                  Zajzon, Barna and
                  Plesser, Hans Ekkehard},
  title        = {NEST 2.14.1},
  month        = sep,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {2.14.1},
  doi          = {10.5281/zenodo.4018724},
  url          = {https://doi.org/10.5281/zenodo.4018724}
}

@software{nest3.0,
  author       = {Hahne, Jan and
                  Diaz, Sandra and
                  Patronis, Alexander and
                  Schenck, Wolfram and
                  Peyser, Alexander and
                  Graber, Steffen and
                  Spreizer, Sebastian and
                  Vennemo, Stine Brekke and
                  Ippen, Tammo and
                  Mørk, Håkon and
                  Jordan, Jakob and
                  Senk, Johanna and
                  Konradi, Sara and
                  Weidel, Philipp and
                  Fardet, Tanguy and
                  Dahmen, David and
                  Terhorst, Dennis and
                  Stapmanns, Jonas and
                  Trensch, Guido and
                  van Meegen, Alexander and
                  Pronold, Jari and
                  Eppler, Jochen Martin and
                  Linssen, Charl and
                  Morrison, Abigail and
                  Sinha, Ankur and
                  Mitchell, Jessica and
                  Kunkel, Susanne and
                  Deepu, Rajalekshmi and
                  Hagen, Espen and
                  Vierjahn, Tom and
                  Kamiji, Nilton Liuji and
                  de Schepper, Robin and
                  Machado, Pedro and
                  Albers, Jasper and
                  Klijn, Wouter and
                  Myczko, Alex and
                  Mayner, William and
                  Nagendra Babu, Pooja and
                  Jiang, Hanjia and
                  Billaudelle, Sebastian and
                  Vogler, Benedikt S. and
                  Miotto, Guilherme and
                  Kusch, Lionel and
                  Antonietti, Alberto and
                  Morales-Gregorio, Aitor and
                  Dolderer, Joris and
                  Bouhadjar, Younes and
                  Plesser, Hans Ekkehard},
  title        = {NEST 3.0},
  month        = jun,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {3.0},
  doi          = {10.5281/zenodo.4739103},
  url          = {https://doi.org/10.5281/zenodo.4739103}
}


@ARTICLE{Albers2022,
AUTHOR={Albers, Jasper and Pronold, Jari and Kurth, Anno Christopher and Vennemo, Stine Brekke and Haghighi Mood, Kaveh and Patronis, Alexander and Terhorst, Dennis and Jordan, Jakob and Kunkel, Susanne and Tetzlaff, Tom and Diesmann, Markus and Senk, Johanna},
TITLE={A Modular Workflow for Performance Benchmarking of Neuronal Network Simulations},
JOURNAL={Frontiers in Neuroinformatics},
VOLUME={16},
YEAR={2022},
URL={https://www.frontiersin.org/articles/10.3389/fninf.2022.837549},
DOI={10.3389/fninf.2022.837549},
ISSN={1662-5196},
ABSTRACT={Modern computational neuroscience strives to develop complex network models to explain dynamics and function of brains in health and disease. This process goes hand in hand with advancements in the theory of neuronal networks and increasing availability of detailed anatomical data on brain connectivity. Large-scale models that study interactions between multiple brain areas with intricate connectivity and investigate phenomena on long time scales such as system-level learning require progress in simulation speed. The corresponding development of state-of-the-art simulation engines relies on information provided by benchmark simulations which assess the time-to-solution for scientifically relevant, complementary network models using various combinations of hardware and software revisions. However, maintaining comparability of benchmark results is difficult due to a lack of standardized specifications for measuring the scaling performance of simulators on high-performance computing (HPC) systems. Motivated by the challenging complexity of benchmarking, we define a generic workflow that decomposes the endeavor into unique segments consisting of separate modules. As a reference implementation for the conceptual workflow, we develop <monospace>beNNch</monospace>: an open-source software framework for the configuration, execution, and analysis of benchmarks for neuronal network simulations. The framework records benchmarking data and metadata in a unified way to foster reproducibility. For illustration, we measure the performance of various versions of the <monospace>NEST</monospace> simulator across network models with different levels of complexity on a contemporary HPC system, demonstrating how performance bottlenecks can be identified, ultimately guiding the development toward more efficient simulation technology.}
}


@article {Morrison2007,
	Title = {Spike-timing-dependent plasticity in balanced random networks},
	Author = {Morrison, Abigail and Aertsen, Ad and Diesmann, Markus},
	DOI = {10.1162/neco.2007.19.6.1437},
	Number = {6},
	Volume = {19},
	Month = {June},
	Year = {2007},
	Journal = {Neural computation},
	ISSN = {0899-7667},
	Pages = {1437—1467},
	Abstract = {The balanced random network model attracts considerable interest because it explains the irregular spiking activity at low rates and large membrane potential fluctuations exhibited by cortical neurons in vivo. In this article, we investigate to what extent this model is also compatible with the experimentally observed phenomenon of spike-timing-dependent plasticity (STDP). Confronted with the plethora of theoretical models for STDP available, we reexamine the experimental data. On this basis, we propose a novel STDP update rule, with a multiplicative dependence on the synaptic weight for depression, and a power law dependence for potentiation. We show that this rule, when implemented in large, balanced networks of realistic connectivity and sparseness, is compatible with the asynchronous irregular activity regime. The resultant equilibrium weight distribution is unimodal with fluctuating individual weight trajectories and does not exhibit development of structure. We investigate the robustness of our results with respect to the relative strength of depression. We introduce synchronous stimulation to a group of neurons and demonstrate that the decoupling of this group from the rest of the network is so severe that it cannot effectively control the spiking of other neurons, even those with the highest convergence from this group.},
	URL = {https://doi.org/10.1162/neco.2007.19.6.1437},
}

@article{Azevedo2009,
  doi = {10.1002/cne.21974},
  url = {https://doi.org/10.1002/cne.21974},
  year = {2009},
  month = apr,
  publisher = {Wiley},
  volume = {513},
  number = {5},
  pages = {532--541},
  author = {Frederico A.C. Azevedo and Ludmila R.B. Carvalho and Lea T. Grinberg and Jos{\'{e}} Marcelo Farfel and Renata E.L. Ferretti and Renata E.P. Leite and Wilson Jacob Filho and Roberto Lent and Suzana Herculano-Houzel},
  title = {Equal numbers of neuronal and nonneuronal cells make the human brain an isometrically scaled-up primate brain},
  journal = {The Journal of Comparative Neurology}
}

@article{Cragg1975,
  title={The density of synapses and neurons in normal, mentally defective ageing human brains.},
  author={Cragg, Brian G},
  journal={Brain},
  volume={98},
  number={1},
  pages={81--90},
  year={1975}
}

@article{Alonso-Nanclares2008,
  title={Gender differences in human cortical synaptic density},
  author={Alonso-Nanclares, Lidia and Gonzalez-Soriano, Juncal and Rodriguez, JR and DeFelipe, Javier},
  journal={Proceedings of the National Academy of Sciences},
  volume={105},
  number={38},
  pages={14615--14619},
  year={2008},
  publisher={National Acad Sciences}
}


@book{Gerstner2014,
author = {Gerstner, Wulfram and Kistler, Werner M. and Naud, Richard and Paninski, Liam},
title = {Neuronal Dynamics: From Single Neurons to Networks and Models of Cognition},
year = {2014},
isbn = {1107635195},
publisher = {Cambridge University Press},
address = {USA},
abstract = {What happens in our brain when we make a decision? What triggers a neuron to send out a signal? What is the neural code? This textbook for advanced undergraduate and beginning graduate students provides a thorough and up-to-date introduction to the fields of computational and theoretical neuroscience. It covers classical topics, including the Hodgkin-Huxley equations and Hopfield model, as well as modern developments in the field such as Generalized Linear Models and decision theory. Concepts are introduced using clear step-by-step explanations suitable for readers with only a basic knowledge of differential equations and probabilities, and are richly illustrated by figures and worked-out examples. End-of-chapter summaries and classroom-tested exercises make the book ideal for courses or for self-study. The authors also give pointers to the literature and an extensive bibliography, which will prove invaluable to readers interested in further study.}
}


@book{DeSchutter2009,
  doi = {10.7551/mitpress/9780262013277.001.0001},
  url = {https://doi.org/10.7551/mitpress/9780262013277.001.0001},
  year = {2009},
  month = sep,
  publisher = {The {MIT} Press},
  editor = {Erik De Schutter},
  title = {Computational Modeling Methods for Neuroscientists}
}


@article{Hodgkin1952,
author = {Hodgkin, A. L. and Huxley, A. F.},
title = {A quantitative description of membrane current and its application to conduction and excitation in nerve},
journal = {The Journal of Physiology},
volume = {117},
number = {4},
pages = {500-544},
doi = {https://doi.org/10.1113/jphysiol.1952.sp004764},
url = {https://physoc.onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1952.sp004764},
eprint = {https://physoc.onlinelibrary.wiley.com/doi/pdf/10.1113/jphysiol.1952.sp004764},
year = {1952}
}

@article{Sboev2016,
author = {Sboev,A.  and Vlasov,D.  and Serenko,A.  and Rybka,R.  and Moloshnikov,I. },
title = {On the applicability of STDP-based learning mechanisms to spiking neuron network models},
journal = {AIP Advances},
volume = {6},
number = {11},
pages = {111305},
year = {2016},
doi = {10.1063/1.4967353},
URL = {https://doi.org/10.1063/1.4967353},
eprint = {https://doi.org/10.1063/1.4967353}
}

@ARTICLE{Sjöström2010,
AUTHOR = {Sjöström, J.  and Gerstner, W. },
TITLE   = {{S}pike-timing dependent plasticity},
YEAR    = {2010},
JOURNAL = {Scholarpedia},
VOLUME  = {5},
NUMBER  = {2},
PAGES   = {1362},
DOI     = {10.4249/scholarpedia.1362},
NOTE    = {revision \#184913}
}

@article{Bi1998,
  doi = {10.1523/jneurosci.18-24-10464.1998},
  url = {https://doi.org/10.1523/jneurosci.18-24-10464.1998},
  year = {1998},
  month = dec,
  publisher = {Society for Neuroscience},
  volume = {18},
  number = {24},
  pages = {10464--10472},
  author = {Guo-qiang Bi and Mu-ming Poo},
  title = {Synaptic Modifications in Cultured Hippocampal Neurons: Dependence on Spike Timing,  Synaptic Strength,  and Postsynaptic Cell Type},
  journal = {The Journal of Neuroscience}
}

@article{Morrison2008,
  doi = {10.1007/s00422-008-0233-1},
  url = {https://doi.org/10.1007/s00422-008-0233-1},
  year = {2008},
  month = may,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {98},
  number = {6},
  pages = {459--478},
  author = {Abigail Morrison and Markus Diesmann and Wulfram Gerstner},
  title = {Phenomenological models of synaptic plasticity based on spike timing},
  journal = {Biological Cybernetics}
}


@book{abbot2005,
  title={Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems},
  author={Dayan, P. and Abbott, L.F.},
  isbn={9780262541855},
  lccn={2001044005},
  series={Computational Neuroscience Series},
  url={https://books.google.it/books?id=fLT4DwAAQBAJ},
  year={2005},
  publisher={MIT Press}
}


@article{sakai2020,
author = {Jill Sakai },
title = {How synaptic pruning shapes neural wiring during development and, possibly, in disease},
journal = {Proceedings of the National Academy of Sciences},
volume = {117},
number = {28},
pages = {16096-16099},
year = {2020},
doi = {10.1073/pnas.2010281117},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2010281117},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2010281117}}

@article{Lamprecht2004,
  doi = {10.1038/nrn1301},
  url = {https://doi.org/10.1038/nrn1301},
  year = {2004},
  month = jan,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {5},
  number = {1},
  pages = {45--54},
  author = {Raphael Lamprecht and Joseph LeDoux},
  title = {Structural plasticity and memory},
  journal = {Nature Reviews Neuroscience}
}

@article{Huttenlocher1979,
title = {Synaptic density in human frontal cortex — Developmental changes and effects of aging},
journal = {Brain Research},
volume = {163},
number = {2},
pages = {195-205},
year = {1979},
issn = {0006-8993},
doi = {https://doi.org/10.1016/0006-8993(79)90349-4},
url = {https://www.sciencedirect.com/science/article/pii/0006899379903494},
author = {Huttenlocher, Peter R.},
abstract = {Density of synaptic profiles in layer 3 of middle frontal gyrus was quantitated in 21 normal human brains ranging from newborn to age 90 years. Synaptic profiles could be reliably demonstrated by the phosphotungstic acid method (Bloom and Aghajanian3) in tissue fixed up to 36 h postmortem. Synaptic density was constant throughout adult life (ages 16–72 years) with a mean of 11.05 × 108synapses/cu.mm ± 0.41 S.E.M.. There was a slight decline in synaptic density in brains of the aged (ages 74–90 years) with a mean of 9.56 × 108synapses/cu.mm ± 0.28S.E.M. in 4 samples (P < 0.05). Synaptic density in neonatal brains was already high—in the range seen in adults. However, synaptic morphology differed; immature profiles had an irregular presynaptic dense band instead of the separate presynaptic projections seen in mature synapses. Synaptic density increased during infancy, reaching a maximum at age 1–2 years which was about 50\% above the adult mean. The decline in synaptic density observed between ages 2–16 years was accompanied by a slight decrease in neuronal density. Human cerebral cortex is one of a number of neuronal systems in which loss of neurons and synapses appears to occur as a late developmental event.}
}

@article{Knoblauch2014,
    doi = {10.1371/journal.pone.0096485},
    author = {Knoblauch, Andreas AND Körner, Edgar AND Körner, Ursula AND Sommer, Friedrich T.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Structural Synaptic Plasticity Has High Memory Capacity and Can Explain Graded Amnesia, Catastrophic Forgetting, and the Spacing Effect},
    year = {2014},
    month = {05},
    volume = {9},
    url = {https://doi.org/10.1371/journal.pone.0096485},
    pages = {1-19},
    abstract = {Although already William James and, more explicitly, Donald Hebb's theory of cell assemblies have suggested that activity-dependent rewiring of neuronal networks is the substrate of learning and memory, over the last six decades most theoretical work on memory has focused on plasticity of existing synapses in prewired networks. Research in the last decade has emphasized that structural modification of synaptic connectivity is common in the adult brain and tightly correlated with learning and memory. Here we present a parsimonious computational model for learning by structural plasticity. The basic modeling units are “potential synapses” defined as locations in the network where synapses can potentially grow to connect two neurons. This model generalizes well-known previous models for associative learning based on weight plasticity. Therefore, existing theory can be applied to analyze how many memories and how much information structural plasticity can store in a synapse. Surprisingly, we find that structural plasticity largely outperforms weight plasticity and can achieve a much higher storage capacity per synapse. The effect of structural plasticity on the structure of sparsely connected networks is quite intuitive: Structural plasticity increases the “effectual network connectivity”, that is, the network wiring that specifically supports storage and recall of the memories. Further, this model of structural plasticity produces gradients of effectual connectivity in the course of learning, thereby explaining various cognitive phenomena including graded amnesia, catastrophic forgetting, and the spacing effect.},
    number = {5},
}

@ARTICLE{Knoblauch2016,
AUTHOR={Knoblauch, Andreas and Sommer, Friedrich T.},
TITLE={Structural Plasticity, Effectual Connectivity, and Memory in Cortex},
JOURNAL={Frontiers in Neuroanatomy},
VOLUME={10},
YEAR={2016},
URL={https://www.frontiersin.org/articles/10.3389/fnana.2016.00063},
DOI={10.3389/fnana.2016.00063},
ISSN={1662-5129},
ABSTRACT={Learning and memory is commonly attributed to the modification of synaptic strengths in neuronal networks. More recent experiments have also revealed a major role of structural plasticity including elimination and regeneration of synapses, growth and retraction of dendritic spines, and remodeling of axons and dendrites. Here we work out the idea that one likely function of structural plasticity is to increase “effectual connectivity” in order to improve the capacity of sparsely connected networks to store Hebbian cell assemblies that are supposed to represent memories. For this we define effectual connectivity as the fraction of synaptically linked neuron pairs within a cell assembly representing a memory. We show by theory and numerical simulation the close links between effectual connectivity and both information storage capacity of neural networks and effective connectivity as commonly employed in functional brain imaging and connectome analysis. Then, by applying our model to a recently proposed memory model, we can give improved estimates on the number of cell assemblies that can be stored in a cortical macrocolumn assuming realistic connectivity. Finally, we derive a simplified model of structural plasticity to enable large scale simulation of memory phenomena, and apply our model to link ongoing adult structural plasticity to recent behavioral data on the spacing effect of learning.}
}

@article{Fu2011,
title = {Experience-dependent structural plasticity in the cortex},
journal = {Trends in Neurosciences},
volume = {34},
number = {4},
pages = {177-187},
year = {2011},
issn = {0166-2236},
doi = {https://doi.org/10.1016/j.tins.2011.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0166223611000178},
author = {Min Fu and Yi Zuo},
abstract = {Synapses are the fundamental units of neuronal circuits. Synaptic plasticity can occur through changes in synaptic strength, as well as through the addition/removal of synapses. Two-photon microscopy in combination with fluorescence labeling offers a powerful tool to peek into the living brain and follow structural reorganization at individual synapses. Time-lapse imaging depicts a dynamic picture in which experience-dependent plasticity of synaptic structures varies between different cortical regions and layers, as well as between neuronal subtypes. Recent studies have demonstrated that the formation and elimination of synaptic structures happens rapidly in a subpopulation of cortical neurons during various sensorimotor learning experiences, and that stabilized synaptic structures are associated with long lasting memories for the task. Therefore, circuit plasticity, mediated by structural remodeling, provides an underlying mechanism for learning and memory.}
}

@article{Butz2009,
title = {Activity-dependent structural plasticity},
journal = {Brain Research Reviews},
volume = {60},
number = {2},
pages = {287-305},
year = {2009},
issn = {0165-0173},
doi = {https://doi.org/10.1016/j.brainresrev.2008.12.023},
url = {https://www.sciencedirect.com/science/article/pii/S0165017308001513},
author = {Markus Butz and Florentin Wörgötter and Arjen {van Ooyen}},
keywords = {Structural Plasticity, Lesion-induced plasticity, Experience-dependent plasticity, Synaptic rewiring, Homeostasis, Activity-dependent neurite outgrowth, Development},
abstract = {Plasticity in the brain reaches far beyond a mere changing of synaptic strengths. Recent time-lapse imaging in the living brain reveals ongoing structural plasticity by forming or breaking of synapses, motile spines, and re-routing of axonal branches in the developing and adult brain. Some forms of structural plasticity do not follow Hebbian- or anti-Hebbian paradigms of plasticity but rather appear to contribute to the homeostasis of network activity. Four decades of lesion studies have brought up a wealth of data on the mutual interdependence of neuronal activity, neurotransmitter release and neuronal morphogenesis and network formation. Here, we review these former studies on structural plasticity in the context of recent experimental studies. We compare spontaneous and experience-dependent structural plasticity with lesion-induced (reactive) structural plasticity that occurs during development and in the adult brain. Understanding the principles of neural network reorganization on a structural level is relevant for a deeper understanding of long-term memory formation as well as for the treatment of neurological diseases such as stroke.}
}

@article{Chklovskii2004,
  doi = {10.1038/nature03012},
  url = {https://doi.org/10.1038/nature03012},
  year = {2004},
  month = oct,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {431},
  number = {7010},
  pages = {782--788},
  author = {D. B. Chklovskii and B. W. Mel and K. Svoboda},
  title = {Cortical rewiring and information storage},
  journal = {Nature}
}

@ARTICLE{Spiess2016,
AUTHOR={Spiess, Robin and George, Richard and Cook, Matthew and Diehl, Peter U.},
TITLE={Structural Plasticity Denoises Responses and Improves Learning Speed},
JOURNAL={Frontiers in Computational Neuroscience},
VOLUME={10},
YEAR={2016},  
URL={https://www.frontiersin.org/articles/10.3389/fncom.2016.00093},
DOI={10.3389/fncom.2016.00093},
ISSN={1662-5188},
ABSTRACT={Despite an abundance of computational models for learning of synaptic weights, there has been relatively little research on structural plasticity, i.e., the creation and elimination of synapses. Especially, it is not clear how structural plasticity works in concert with spike-timing-dependent plasticity (STDP) and what advantages their combination offers. Here we present a fairly large-scale functional model that uses leaky integrate-and-fire neurons, STDP, homeostasis, recurrent connections, and structural plasticity to learn the input encoding, the relation between inputs, and to infer missing inputs. Using this model, we compare the error and the amount of noise in the network's responses with and without structural plasticity and the influence of structural plasticity on the learning speed of the network. Using structural plasticity during learning shows good results for learning the representation of input values, i.e., structural plasticity strongly reduces the noise of the response by preventing spikes with a high error. For inferring missing inputs we see similar results, with responses having less noise if the network was trained using structural plasticity. Additionally, using structural plasticity with pruning significantly decreased the time to learn weights suitable for inference. Presumably, this is due to the clearer signal containing less spikes that misrepresent the desired value. Therefore, this work shows that structural plasticity is not only able to improve upon the performance using STDP without structural plasticity but also speeds up learning. Additionally, it addresses the practical problem of limited resources for connectivity that is not only apparent in the mammalian neocortex but also in computer hardware or neuromorphic (brain-inspired) hardware by efficiently pruning synapses without losing performance.}
}

@article{Navlakha2015,
    doi = {10.1371/journal.pcbi.1004347},
    author = {Navlakha, Saket AND Barth, Alison L. AND Bar-Joseph, Ziv},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Decreasing-Rate Pruning Optimizes the Construction of Efficient and Robust Distributed Networks},
    year = {2015},
    month = {07},
    volume = {11},
    url = {https://doi.org/10.1371/journal.pcbi.1004347},
    pages = {1-23},
    abstract = {Robust, efficient, and low-cost networks are advantageous in both biological and engineered systems. During neural network development in the brain, synapses are massively over-produced and then pruned-back over time. This strategy is not commonly used when designing engineered networks, since adding connections that will soon be removed is considered wasteful. Here, we show that for large distributed routing networks, network function is markedly enhanced by hyper-connectivity followed by aggressive pruning and that the global rate of pruning, a developmental parameter not previously studied by experimentalists, plays a critical role in optimizing network structure. We first used high-throughput image analysis techniques to quantify the rate of pruning in the mammalian neocortex across a broad developmental time window and found that the rate is decreasing over time. Based on these results, we analyzed a model of computational routing networks and show using both theoretical analysis and simulations that decreasing rates lead to more robust and efficient networks compared to other rates. We also present an application of this strategy to improve the distributed design of airline networks. Thus, inspiration from neural network formation suggests effective ways to design distributed networks across several domains.},
    number = {7},
}

@article{Moyer2015,
title = {Dendritic spine alterations in schizophrenia},
journal = {Neuroscience Letters},
volume = {601},
pages = {46-53},
year = {2015},
note = {Dendritic Spine Dysgenesis in Neuropsychiatric Disease},
issn = {0304-3940},
doi = {https://doi.org/10.1016/j.neulet.2014.11.042},
url = {https://www.sciencedirect.com/science/article/pii/S0304394014009203},
author = {Caitlin E. Moyer and Micah A. Shelton and Robert A. Sweet},
keywords = {Schizophrenia, Dendritic spine, Postmortem, Adolescence, MAP2, Kalirin},
abstract = {Schizophrenia is a chronic illness affecting approximately 0.5–1\% of the world’s population. The etiology of schizophrenia is complex, including multiple genes, and contributing environmental effects that adversely impact neurodevelopment. Nevertheless, a final common result, present in many subjects with schizophrenia, is impairment of pyramidal neuron dendritic morphology in multiple regions of the cerebral cortex. In this review, we summarize the evidence of reduced dendritic spine density and other dendritic abnormalities in schizophrenia, evaluate current data that informs the neurodevelopment timing of these impairments, and discuss what is known about possible upstream sources of dendritic spine loss in this illness.}
}

@article{Bourgeron2009,
title = {A synaptic trek to autism},
journal = {Current Opinion in Neurobiology},
volume = {19},
number = {2},
pages = {231-234},
year = {2009},
note = {Development},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2009.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0959438809000592},
author = {Thomas Bourgeron},
abstract = {Autism spectrum disorders (ASD) are diagnosed on the basis of three behavioral features namely deficits in social communication, absence or delay in language, and stereotypy. The susceptibility genes to ASD remain largely unknown, but two major pathways are emerging. Mutations in TSC1/TSC2, NF1, or PTEN activate the mTOR/PI3K pathway and lead to syndromic ASD with tuberous sclerosis, neurofibromatosis, or macrocephaly. Mutations in NLGN3/4, SHANK3, or NRXN1 alter synaptic function and lead to mental retardation, typical autism, or Asperger syndrome. The mTOR/PI3K pathway is associated with abnormal cellular/synaptic growth rate, whereas the NRXN–NLGN–SHANK pathway is associated with synaptogenesis and imbalance between excitatory and inhibitory currents. Taken together, these data strongly suggest that abnormal synaptic homeostasis represent a risk factor to ASD.}
}

@article{Hutsler2010,
title = {Increased dendritic spine densities on cortical projection neurons in autism spectrum disorders},
journal = {Brain Research},
volume = {1309},
pages = {83-94},
year = {2010},
issn = {0006-8993},
doi = {https://doi.org/10.1016/j.brainres.2009.09.120},
url = {https://www.sciencedirect.com/science/article/pii/S0006899309023117},
author = {Jeffrey J. Hutsler and Hong Zhang},
keywords = {Cerebral cortex, Autistic disorder, Pyramidal cell, Dendritic spine, Neuroanatomy},
abstract = {Multiple types of indirect evidence have been used to support theories of altered cortical connectivity in autism spectrum disorders (ASD). In other developmental disorders reduced spine expression is commonly found, while conditions such as fragile X syndrome show increased spine densities. Despite its relevance to theories of altered cortical connectivity, synaptic spine expression has not been systematically explored in ASD. Here we examine dendritic spines on Golgi-impregnated cortical pyramidal cells in the cortex of ASD subjects and age-matched control cases. Pyramidal cells were studied within both the superficial and deep cortical layers of frontal, temporal, and parietal lobe regions. Relative to controls, spine densities were greater in ASD subjects. In analyses restricted to the apical dendrites of pyramidal cells, greater spine densities were found predominantly within layer II of each cortical location and within layer V of the temporal lobe. High spine densities were associated with decreased brain weights and were most commonly found in ASD subjects with lower levels of cognitive functioning. Greater spine densities in ASD subjects provide structural support for recent suggestions of connectional changes within the cerebral cortex that may result in altered cortical computations.}
}

@article{Pagani2021,
  doi = {10.1038/s41467-021-26131-z},
  url = {https://doi.org/10.1038/s41467-021-26131-z},
  year = {2021},
  month = oct,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {12},
  number = {1},
  author = {Marco Pagani and Noemi Barsotti and Alice Bertero and Stavros Trakoshis and Laura Ulysse and Andrea Locarno and Ieva Miseviciute and Alessia De Felice and Carola Canella and Kaustubh Supekar and Alberto Galbusera and Vinod Menon and Raffaella Tonini and Gustavo Deco and Michael V. Lombardo and Massimo Pasqualetti and Alessandro Gozzi},
  title = {{mTOR}-related synaptic pathology causes autism spectrum disorder-associated functional hyperconnectivity},
  journal = {Nature Communications}
}

@article{Glantz2000,
    author = {Glantz, Leisa A. and Lewis, David A.},
    title = "{Decreased Dendritic Spine Density on Prefrontal Cortical Pyramidal Neurons in Schizophrenia}",
    journal = {Archives of General Psychiatry},
    volume = {57},
    number = {1},
    pages = {65-73},
    year = {2000},
    month = {01},
    abstract = "{The pathophysiological characteristics of schizophrenia appear to involve altered synaptic connectivity in the dorsolateral prefrontal cortex. Given the central role that layer 3 pyramidal neurons play in corticocortical and thalamocortical connectivity, we hypothesized that the excitatory inputs to these neurons are altered in subjects with schizophrenia.To test this hypothesis, we determined the density of dendritic spines, markers of excitatory inputs, on the basilar dendrites of Golgi-impregnated pyramidal neurons in the superficial and deep portions of layer 3 in the dorsolateral prefrontal cortex (area 46) and in layer 3 of the primary visual cortex (area 17) of 15 schizophrenic subjects, 15 normal control subjects, and 15 nonschizophrenic subjects with a psychiatric illness (referred to as psychiatric subjects).There was a significant effect of diagnosis on spine density only for deep layer 3 pyramidal neurons in area 46 (P = .006). In the schizophrenic subjects, spine density on these neurons was decreased by 23\% and 16\% compared with the normal control (P = .004) and psychiatric (P = .08) subjects, respectively. In contrast, spine density on neurons in superficial layer 3 in area 46 (P = .09) or in area 17 (P = .08) did not significantly differ across the 3 subject groups. Furthermore, spine density on deep layer 3 neurons in area 46 did not significantly (P = .81) differ between psychiatric subjects treated with antipsychotic agents and normal controls.This region- and disease-specific decrease in dendritic spine density on dorsolateral prefrontal cortex layer 3 pyramidal cells is consistent with the hypothesis that the number of cortical and/or thalamic excitatory inputs to these neurons is altered in subjects with schizophrenia.Arch Gen Psychiatry. 2000;57:65-73-->}",
    issn = {0003-990X},
    doi = {10.1001/archpsyc.57.1.65},
    url = {https://doi.org/10.1001/archpsyc.57.1.65},
    eprint = {https://jamanetwork.com/journals/jamapsychiatry/articlepdf/481552/yoa9030.pdf},
}


@article{Mattson1988,
  doi = {10.1016/0165-0173(88)90020-3},
  url = {https://doi.org/10.1016/0165-0173(88)90020-3},
  year = {1988},
  month = apr,
  publisher = {Elsevier {BV}},
  volume = {13},
  number = {2},
  pages = {179--212},
  author = {Mark P. Mattson},
  title = {Neurotransmitters in the regulation of neuronal cytoarchitecture},
  journal = {Brain Research Reviews}
}


@article{Richards2005,
  doi = {10.1073/pnas.0501881102},
  url = {https://doi.org/10.1073/pnas.0501881102},
  year = {2005},
  month = apr,
  publisher = {Proceedings of the National Academy of Sciences},
  volume = {102},
  number = {17},
  pages = {6166--6171},
  author = {David A. Richards and Jos{\'{e}} Maria Mateos and Sylvain Hugel and Vincenzo de Paola and Pico Caroni and Beat H. G\"{a}hwiler and R. Anne McKinney},
  title = {Glutamate induces the rapid formation of spine head protrusions in hippocampal slice cultures},
  journal = {Proceedings of the National Academy of Sciences}
}


@ARTICLE{Fauth2016,
AUTHOR={Fauth, Michael and Tetzlaff, Christian},
TITLE={Opposing Effects of Neuronal Activity on Structural Plasticity},
JOURNAL={Frontiers in Neuroanatomy},
VOLUME={10},
YEAR={2016},
URL={https://www.frontiersin.org/articles/10.3389/fnana.2016.00075},
DOI={10.3389/fnana.2016.00075},
ISSN={1662-5129},
ABSTRACT={The connectivity of the brain is continuously adjusted to new environmental influences by several activity-dependent adaptive processes. The most investigated adaptive mechanism is activity-dependent functional or synaptic plasticity regulating the transmission efficacy of existing synapses. Another important but less prominently discussed adaptive process is structural plasticity, which changes the connectivity by the formation and deletion of synapses. In this review, we show, based on experimental evidence, that structural plasticity can be classified similar to synaptic plasticity into two categories: (i) Hebbian structural plasticity, which leads to an increase (decrease) of the number of synapses during phases of high (low) neuronal activity and (ii) homeostatic structural plasticity, which balances these changes by removing and adding synapses. Furthermore, based on experimental and theoretical insights, we argue that each type of structural plasticity fulfills a different function. While Hebbian structural changes enhance memory lifetime, storage capacity, and memory robustness, homeostatic structural plasticity self-organizes the connectivity of the neural network to assure stability. However, the link between functional synaptic and structural plasticity as well as the detailed interactions between Hebbian and homeostatic structural plasticity are more complex. This implies even richer dynamics requiring further experimental and theoretical investigations.}
}

@article{Kasai2003,
  doi = {10.1016/s0166-2236(03)00162-0},
  url = {https://doi.org/10.1016/s0166-2236(03)00162-0},
  year = {2003},
  month = jul,
  publisher = {Elsevier {BV}},
  volume = {26},
  number = {7},
  pages = {360--368},
  author = {Haruo Kasai and Masanori Matsuzaki and Jun Noguchi and Nobuaki Yasumatsu and Hiroyuki Nakahara},
  title = {Structure{\textendash}stability{\textendash}function relationships of dendritic spines},
  journal = {Trends in Neurosciences}
}

@article{Hill2013,
  doi = {10.1523/jneurosci.1404-12.2013},
  url = {https://doi.org/10.1523/jneurosci.1404-12.2013},
  year = {2013},
  month = jan,
  publisher = {Society for Neuroscience},
  volume = {33},
  number = {2},
  pages = {678--686},
  author = {Travis C. Hill and Karen Zito},
  title = {{LTP}-Induced Long-Term Stabilization of Individual Nascent Dendritic Spines},
  journal = {The Journal of Neuroscience}
}

@article{Toni1999,
  doi = {10.1038/46574},
  url = {https://doi.org/10.1038/46574},
  year = {1999},
  month = nov,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {402},
  number = {6760},
  pages = {421--425},
  author = {N. Toni and P.-A. Buchs and I. Nikonenko and C. R. Bron and D. Muller},
  title = {{LTP} promotes formation of multiple spine synapses between a single axon~terminal and a dendrite},
  journal = {Nature}
}


@article{Wiegert2013,
author = {J. Simon Wiegert  and Thomas G. Oertner },
title = {Long-term depression triggers the selective elimination of weakly integrated synapses},
journal = {Proceedings of the National Academy of Sciences},
volume = {110},
number = {47},
pages = {E4510-E4519},
year = {2013},
doi = {10.1073/pnas.1315926110},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1315926110},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1315926110},
abstract = {Long-term depression (LTD) weakens synaptic transmission in an activity-dependent manner. It is not clear, however, whether individual synapses are able to maintain a depressed state indefinitely, as intracellular recordings rarely exceed 1 h. Here, we combine optogenetic stimulation of identified Schaffer collateral axons with two-photon imaging of postsynaptic calcium signals and follow the fate of individual synapses for 7 d after LTD induction. Optogenetic stimulation of CA3 pyramidal cells at 1 Hz led to strong and reliable depression of postsynaptic calcium transients in CA1. NMDA receptor activation was necessary for successful induction of LTD. We found that, in the days following LTD, many depressed synapses and their “neighbors” were eliminated from the hippocampal circuit. The average lifetime of synapses on nonstimulated dendritic branches of the same neurons remained unaffected. Persistence of individual depressed synapses was highly correlated with reliability of synaptic transmission, but not with spine size or the amplitude of spine calcium transients. Our data suggest that LTD initially leads to homogeneous depression of synaptic function, followed by selective removal of unreliable synapses and recovery of function in the persistent fraction.}}


@ARTICLE{Tiddia2022_WM,
AUTHOR={Tiddia, Gianmarco and Golosio, Bruno and Fanti, Viviana and Paolucci, Pier Stanislao},
TITLE={Simulations of working memory spiking networks driven by short-term plasticity},
JOURNAL={Frontiers in Integrative Neuroscience},
VOLUME={16},
YEAR={2022},
URL={https://www.frontiersin.org/articles/10.3389/fnint.2022.972055},
DOI={10.3389/fnint.2022.972055},
ISSN={1662-5145},
ABSTRACT={Working Memory (WM) is a cognitive mechanism that enables temporary holding and manipulation of information in the human brain. This mechanism is mainly characterized by a neuronal activity during which neuron populations are able to maintain an enhanced spiking activity after being triggered by a short external cue. In this study, we implement, using the NEST simulator, a spiking neural network model in which the WM activity is sustained by a mechanism of short-term synaptic facilitation related to presynaptic calcium kinetics. The model, which is characterized by leaky integrate-and-fire neurons with exponential postsynaptic currents, is able to autonomously show an activity regime in which the memory information can be stored in a synaptic form as a result of synaptic facilitation, with spiking activity functional to facilitation maintenance. The network is able to simultaneously keep multiple memories by showing an alternated synchronous activity which preserves the synaptic facilitation within the neuron populations holding memory information. The results shown in this study confirm that a WM mechanism can be sustained by synaptic facilitation.}
}


@misc{Tiddia2023,
      title={A theoretical framework for learning through structural plasticity}, 
      author={Gianmarco Tiddia and Luca Sergi and Bruno Golosio},
      year={2023},
      eprint={2307.11735},
      archivePrefix={arXiv},
      primaryClass={q-bio.NC}
}


@article {Roxin2011,
	author = {Alex Roxin and Nicolas Brunel and David Hansel and Gianluigi Mongillo and Carl van Vreeswijk},
	title = {On the Distribution of Firing Rates in Networks of Cortical Neurons},
	volume = {31},
	number = {45},
	pages = {16217--16226},
	year = {2011},
	doi = {10.1523/JNEUROSCI.1677-11.2011},
	publisher = {Society for Neuroscience},
	abstract = {The distribution of in vivo average firing rates within local cortical networks has been reported to be highly skewed and long tailed. The distribution of average single-cell inputs, conversely, is expected to be Gaussian by the central limit theorem. This raises the issue of how a skewed distribution of firing rates might result from a symmetric distribution of inputs. We argue that skewed rate distributions are a signature of the nonlinearity of the in vivo f{\textendash}I curve. During in vivo conditions, ongoing synaptic activity produces significant fluctuations in the membrane potential of neurons, resulting in an expansive nonlinearity of the f{\textendash}I curve for low and moderate inputs. Here, we investigate the effects of single-cell and network parameters on the shape of the f{\textendash}I curve and, by extension, on the distribution of firing rates in randomly connected networks.},
	issn = {0270-6474},
	URL = {https://www.jneurosci.org/content/31/45/16217},
	eprint = {https://www.jneurosci.org/content/31/45/16217.full.pdf},
	journal = {Journal of Neuroscience}
}

@article{Rigotti2013,
  doi = {10.1038/nature12160},
  url = {https://doi.org/10.1038/nature12160},
  year = {2013},
  month = may,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {497},
  number = {7451},
  pages = {585--590},
  author = {Mattia Rigotti and Omri Barak and Melissa R. Warden and Xiao-Jing Wang and Nathaniel D. Daw and Earl K. Miller and Stefano Fusi},
  title = {The importance of mixed selectivity in complex cognitive tasks},
  journal = {Nature}
}


@article{Fusi2008,
  doi = {10.1126/science.1155914},
  url = {https://doi.org/10.1126/science.1155914},
  year = {2008},
  month = mar,
  publisher = {American Association for the Advancement of Science ({AAAS})},
  volume = {319},
  number = {5869},
  pages = {1495--1496},
  author = {Stefano Fusi},
  title = {A Quiescent Working Memory},
  journal = {Science}
}


@article{Attwell2001,
author = {David Attwell and Simon B. Laughlin},
title ={An Energy Budget for Signaling in the Grey Matter of the Brain},
journal = {Journal of Cerebral Blood Flow \& Metabolism},
volume = {21},
number = {10},
pages = {1133-1145},
year = {2001},
doi = {10.1097/00004647-200110000-00001},
    note ={PMID: 11598490},

URL = { 
        https://doi.org/10.1097/00004647-200110000-00001
    
},
eprint = { 
        https://doi.org/10.1097/00004647-200110000-00001
    
}
,
    abstract = { Anatomic and physiologic data are used to analyze the energy expenditure on different components of excitatory signaling in the grey matter of rodent brain. Action potentials and postsynaptic effects of glutamate are predicted to consume much of the energy (47\% and 34\%, respectively), with the resting potential consuming a smaller amount (13\%), and glutamate recycling using only 3\%. Energy usage depends strongly on action potential rate—an increase in activity of 1 action potential/cortical neuron/s will raise oxygen consumption by 145 mL/100 g grey matter/h. The energy expended on signaling is a large fraction of the total energy used by the brain; this favors the use of energy efficient neural codes and wiring patterns. Our estimates of energy usage predict the use of distributed codes, with ≤15\% of neurons simultaneously active, to reduce energy consumption and allow greater computing power from a fixed number of neurons. Functional magnetic resonance imaging signals are likely to be dominated by changes in energy usage associated with synaptic currents and action potential propagation. }
}

@article{Lennie2003,
title = {The Cost of Cortical Computation},
journal = {Current Biology},
volume = {13},
number = {6},
pages = {493-497},
year = {2003},
issn = {0960-9822},
doi = {https://doi.org/10.1016/S0960-9822(03)00135-0},
url = {https://www.sciencedirect.com/science/article/pii/S0960982203001350},
author = {Peter Lennie},
abstract = {Electrophysiological recordings show that individual neurons in cortex are strongly activated when engaged in appropriate tasks, but they tell us little about how many neurons might be engaged by a task, which is important to know if we are to understand how cortex encodes information. For human cortex, I estimate the cost of individual spikes, then, from the known energy consumption of cortex, I establish how many neurons can be active concurrently. The cost of a single spike is high, and this severely limits, possibly to fewer than 1%, the number of neurons that can be substantially active concurrently. The high cost of spikes requires the brain not only to use representational codes that rely on very few active neurons, but also to allocate its energy resources flexibly among cortical regions according to task demand. The latter constraint explains the investment in local control of hemodynamics, exploited by functional magnetic resonance imaging, and the need for mechanisms of selective attention.}
}



@article{Bouhadjar2022,
    doi = {10.1371/journal.pcbi.1010233},
    author = {Bouhadjar, Younes AND Wouters, Dirk J. AND Diesmann, Markus AND Tetzlaff, Tom},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Sequence learning, prediction, and replay in networks of spiking neurons},
    year = {2022},
    month = {06},
    volume = {18},
    url = {https://doi.org/10.1371/journal.pcbi.1010233},
    pages = {1-36},
    abstract = {Sequence learning, prediction and replay have been proposed to constitute the universal computations performed by the neocortex. The Hierarchical Temporal Memory (HTM) algorithm realizes these forms of computation. It learns sequences in an unsupervised and continuous manner using local learning rules, permits a context specific prediction of future sequence elements, and generates mismatch signals in case the predictions are not met. While the HTM algorithm accounts for a number of biological features such as topographic receptive fields, nonlinear dendritic processing, and sparse connectivity, it is based on abstract discrete-time neuron and synapse dynamics, as well as on plasticity mechanisms that can only partly be related to known biological mechanisms. Here, we devise a continuous-time implementation of the temporal-memory (TM) component of the HTM algorithm, which is based on a recurrent network of spiking neurons with biophysically interpretable variables and parameters. The model learns high-order sequences by means of a structural Hebbian synaptic plasticity mechanism supplemented with a rate-based homeostatic control. In combination with nonlinear dendritic input integration and local inhibitory feedback, this type of plasticity leads to the dynamic self-organization of narrow sequence-specific subnetworks. These subnetworks provide the substrate for a faithful propagation of sparse, synchronous activity, and, thereby, for a robust, context specific prediction of future sequence elements as well as for the autonomous replay of previously learned sequences. By strengthening the link to biology, our implementation facilitates the evaluation of the TM hypothesis based on experimentally accessible quantities. The continuous-time implementation of the TM algorithm permits, in particular, an investigation of the role of sequence timing for sequence learning, prediction and replay. We demonstrate this aspect by studying the effect of the sequence speed on the sequence learning performance and on the speed of autonomous sequence replay.},
    number = {6},
}


@article{hawkins2011cortical,
  title={Cortical learning algorithm and hierarchical temporal memory},
  author={Hawkins, Jeff and Ahmad, Subutai and Dubinsky, Donna and others},
  journal={Numenta Whitepaper},
  volume={1},
  number={68},
  pages={2},
  year={2011}
}


@article{Noudoost2010,
title = {Top-down control of visual attention},
journal = {Current Opinion in Neurobiology},
volume = {20},
number = {2},
pages = {183-190},
year = {2010},
note = {Cognitive neuroscience},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2010.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0959438810000255},
author = {Behrad Noudoost and Mindy H Chang and Nicholas A Steinmetz and Tirin Moore},
abstract = {Top-down visual attention improves perception of selected stimuli and that improvement is reflected in the neural activity at many stages throughout the visual system. Recent studies of top-down attention have elaborated on the signatures of its effects within visual cortex and have begun identifying its causal basis. Evidence from these studies suggests that the correlates of spatial attention exhibited by neurons within the visual system originate from a distributed network of structures involved in the programming of saccadic eye movements. We summarize this evidence and discuss its relationship to the neural mechanisms of spatial working memory.}
}


@ARTICLE{NEST,
  author  = {Marc-Oliver Gewaltig and Markus Diesmann},
  title   = {NEST (NEural Simulation Tool)},
  journal = {Scholarpedia},
  year    = {2007},
  volume  = {2},
  pages   = {1430},
  number  = {4}
}


@mastersthesis{Sergi2023,
  author  = "Luca Sergi",
  title   = "Teoria di campo medio dell'apprendimento nei sistemi neurali biologici attraverso la plasticità sinaptica strutturale",
  school  = "Department of Physics, University of Cagliari, Italy",
  month   = "July",
  year    = "2023",
  type    = "Master's Thesis",
  note    = "",
  annote  = ""
}


@article{Butts2007,
  doi = {10.1371/journal.pbio.0050061},
  url = {https://doi.org/10.1371/journal.pbio.0050061},
  year = {2007},
  month = mar,
  publisher = {Public Library of Science ({PLoS})},
  volume = {5},
  number = {3},
  pages = {e61},
  author = {Daniel A Butts and Patrick O Kanold and Carla J Shatz},
  editor = {Charles F Stevens},
  title = {A Burst-Based {\textquotedblleft}Hebbian{\textquotedblright} Learning Rule at Retinogeniculate Synapses Links Retinal Waves to Activity-Dependent Refinement},
  journal = {{PLoS} Biology}
}

@article{Payeur2021,
  doi = {10.1038/s41593-021-00857-x},
  url = {https://doi.org/10.1038/s41593-021-00857-x},
  year = {2021},
  month = may,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {24},
  number = {7},
  pages = {1010--1019},
  author = {Alexandre Payeur and Jordan Guerguiev and Friedemann Zenke and Blake A. Richards and Richard Naud},
  title = {Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits},
  journal = {Nature Neuroscience}
}


@article{Hass2022,
title = {Constraints on persistent activity in a biologically detailed network model of the prefrontal cortex with heterogeneities},
journal = {Progress in Neurobiology},
volume = {215},
pages = {102287},
year = {2022},
issn = {0301-0082},
doi = {https://doi.org/10.1016/j.pneurobio.2022.102287},
url = {https://www.sciencedirect.com/science/article/pii/S0301008222000739},
author = {Joachim Hass and Salva Ardid and Jason Sherfey and Nancy Kopell},
keywords = {Persistent activity, Working memory, Prefrontal cortex, Data-driven network model, Heterogeneity, Irregular activity},
abstract = {Persistent activity, the maintenance of neural activation over short periods of time in cortical networks, is widely thought to underlie the cognitive function of working memory. A large body of modeling studies has reproduced this kind of activity using cell assemblies with strengthened synaptic connections. However, almost all of these studies have considered persistent activity within networks with homogeneous neurons and synapses, making it difficult to judge the validity of such model results for cortical dynamics, which is based on highly heterogeneous neurons. Here, we consider persistent activity in a detailed, strongly data-driven network model of the prefrontal cortex with heterogeneous neuron and synapse parameters. Surprisingly, persistent activity could not be reproduced in this model without incorporating further constraints. We identified three factors that prevent successful persistent activity: heterogeneity in the cell parameters of interneurons, heterogeneity in the parameters of short-term synaptic plasticity and heterogeneity in the synaptic weights. We also discovered a general dynamic mechanism that prevents persistent activity in the presence of heterogeneities, namely a gradual drop-out of cell assembly neurons out of a bistable regime as input variability increases. Based on this mechanism, we found that persistent activity is recovered if heterogeneity is compensated, e.g., by a homeostatic plasticity mechanism. Cell assemblies shaped in this way may be potentially targeted by distinct inputs or become more responsive to specific tuning or spectral properties. Finally, we show that persistent activity in the model is robust against external noise, but the compensation of heterogeneities may prevent the dynamic generation of intrinsic in vivo-like irregular activity. These results may help informing the ongoing debate about the neural basis of working memory.}
}


@article{DePitta2022,
author = {Maurizio De Pittà  and Nicolas Brunel },
title = {Multiple forms of working memory emerge from synapse–astrocyte interactions in a neuron–glia network model},
journal = {Proceedings of the National Academy of Sciences},
volume = {119},
number = {43},
pages = {e2207912119},
year = {2022},
doi = {10.1073/pnas.2207912119},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2207912119},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2207912119},
abstract = {Persistent activity in populations of neurons, time-varying activity across a neural population, or activity-silent mechanisms carried out by hidden internal states of the neural population have been proposed as different mechanisms of working memory (WM). Whether these mechanisms could be mutually exclusive or occur in the same neuronal circuit remains, however, elusive, and so do their biophysical underpinnings. While WM is traditionally regarded to depend purely on neuronal mechanisms, cortical networks also include astrocytes that can modulate neural activity. We propose and investigate a network model that includes both neurons and glia and show that glia–synapse interactions can lead to multiple stable states of synaptic transmission. Depending on parameters, these interactions can lead in turn to distinct patterns of network activity that can serve as substrates for WM.}}


@article{Becker2022,
    doi = {10.1371/journal.pcbi.1010543},
    author = {Becker, Sophia AND Nold, Andreas AND Tchumatchenko, Tatjana},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Modulation of working memory duration by synaptic and astrocytic mechanisms},
    year = {2022},
    month = {10},
    volume = {18},
    url = {https://doi.org/10.1371/journal.pcbi.1010543},
    pages = {1-25},
    abstract = {Short-term synaptic plasticity and modulations of the presynaptic vesicle release rate are key components of many working memory models. At the same time, an increasing number of studies suggests a potential role of astrocytes in modulating higher cognitive function such as WM through their influence on synaptic transmission. Which influence astrocytic signaling could have on the stability and duration of WM representations, however, is still unclear. Here, we introduce a slow, activity-dependent astrocytic regulation of the presynaptic release probability in a synaptic attractor model of WM. We compare and analyze simulations of a simple WM protocol in firing rate and spiking networks with and without astrocytic regulation, and underpin our observations with analyses of the phase space dynamics in the rate network. We find that the duration and stability of working memory representations are altered by astrocytic signaling and by noise. We show that astrocytic signaling modulates the mean duration of WM representations. Moreover, if the astrocytic regulation is strong, a slow presynaptic timescale introduces a ‘window of vulnerability’, during which WM representations are easily disruptable by noise before being stabilized. We identify two mechanisms through which noise from different sources in the network can either stabilize or destabilize WM representations. Our findings suggest that (i) astrocytic regulation can act as a crucial determinant for the duration of WM representations in synaptic attractor models of WM, and (ii) that astrocytic signaling could facilitate different mechanisms for volitional top-down control of WM representations and their duration.},
    number = {10},
}


@article{Kamiski2019,
  doi = {10.1111/nyas.14213},
  url = {https://doi.org/10.1111/nyas.14213},
  year = {2019},
  month = aug,
  publisher = {Wiley},
  volume = {1464},
  number = {1},
  pages = {64--75},
  author = {Jan Kami{\'{n}}ski and Ueli Rutishauser},
  title = {Between persistently active and activity-silent frameworks: novel vistas on the cellular basis of working memory},
  journal = {Annals of the New York Academy of Sciences}
}


@article{Masse2019,
  doi = {10.1038/s41593-019-0414-3},
  url = {https://doi.org/10.1038/s41593-019-0414-3},
  year = {2019},
  month = jun,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {22},
  number = {7},
  pages = {1159--1167},
  author = {Nicolas Y. Masse and Guangyu R. Yang and H. Francis Song and Xiao-Jing Wang and David J. Freedman},
  title = {Circuit mechanisms for the maintenance and manipulation of information in working memory},
  journal = {Nature Neuroscience}
}


@article{Brunel2007,
  doi = {10.1007/s00422-007-0190-0},
  url = {https://doi.org/10.1007/s00422-007-0190-0},
  year = {2007},
  month = oct,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {97},
  number = {5-6},
  pages = {337--339},
  author = {Nicolas Brunel and Mark C. W. van Rossum},
  title = {Lapicque's 1907 paper: from frogs to integrate-and-fire},
  journal = {Biological Cybernetics}
}

@article{Brunel2007_lapique,
  doi = {10.1007/s00422-007-0189-6},
  url = {https://doi.org/10.1007/s00422-007-0189-6},
  year = {2007},
  month = nov,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {97},
  number = {5-6},
  pages = {341--349},
  author = {Nicolas Brunel and Mark C. W. van Rossum},
  title = {Quantitative investigations of electrical nerve excitation treated as polarization},
  journal = {Biological Cybernetics}
}


